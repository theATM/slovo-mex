{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Modal Data Fusion - Project Work: Multi-Modal Physical Exercise Classification\n",
    "\n",
    "\n",
    "In this project, real multi-modal data is studied by utilizing different techniques presented during the course. In addition, there is an optional task to try some different approaches to identify persons from the same dataset. Open MEx dataset from UCI machine learning repository is used. Idea is to apply different techniques to recognize physical exercises from wearable sensors and depth camera, user-independently."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Author(s)\n",
    "Add your information here\n",
    "\n",
    "Name(s):\n",
    "Aleksander Madajczak,\n",
    "Jan Fabian\n",
    "\n",
    "Student number(s):\n",
    "2207367\n",
    "2207371\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "\n",
    "The goal of this project is to develop user-independent pre-processing and classification models to recognize 7 different physical exercises measured by accelerometer (attached to subject's thigh) and depth camera (above the subject facing downwards recording an aerial view). All the exercises were performed subject lying down on the mat. Original dataset have also another acceleration sensor and pressure-sensitive mat, but those two modalities are ommited in this project. There are totally 30 subjects in the original dataset, and in this work subset of 10 person is utilized. Detailed description of the dataset and original data can be access in [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#). We are providing the subset of dataset in Moodle.\n",
    "\n",
    "The project work is divided on following phases:\n",
    "\n",
    "1. Data preparation, exploration, and visualization\n",
    "2. Feature extraction and unimodal fusion for classification\n",
    "3. Feature extraction and feature-level fusion for multimodal classification\n",
    "4. Decision-level fusion for multimodal classification\n",
    "5. Bonus task: Multimodal biometric identification of persons\n",
    "\n",
    "where 1-4 are compulsory (max. 10 points each), and 5 is optional to get bonus points (max. 5+5 points). In each phase, you should visualize and analyse the results and document the work and findings properly by text blocks and figures between the code. <b> Nice looking </b> and <b> informative </b> notebook representing your results and analysis will be part of the grading in addition to actual implementation.\n",
    "\n",
    "The results are validated using confusion matrices and F1 scores. F1 macro score is given as\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{equation}\n",
    "F1_{macro} = \\frac{1}{N} \\sum_i^N F1_i,\n",
    "\\end{equation}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "where $F1_i = 2  \\frac{precision_i * recall_i}{precision_i + recall_i}$, and $N$ is the number of labels.\n",
    "<br>\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "After the project work, you should\n",
    "\n",
    "- be able to study real world multi-modal data\n",
    "- be able to apply different data fusion techniques to real-world problem\n",
    "- be able to evaluate the results\n",
    "- be able to analyse the outcome\n",
    "- be able to document your work properly\n",
    "\n",
    "## Relevant lectures\n",
    "\n",
    "Lectures 1-8\n",
    "\n",
    "## Relevant exercises\n",
    "\n",
    "Exercises 0-6\n",
    "\n",
    "## Relevant chapters in course book\n",
    "\n",
    "Chapter 1-14\n",
    "\n",
    "## Additional Material\n",
    "\n",
    "* Original dataset [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#)\n",
    "* Related scientific article [MEx: Multi-modal Exercises Dataset for Human Activity Recognition](https://arxiv.org/pdf/1908.08992.pdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='task1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 1.</b>\n",
    "\n",
    "Download data from the Moodle's Project section. Get yourself familiar with the folder structure and data. You can read the data files using the function given below. Each file consists one exercise type performed by single user. Data are divided on multiple folders. Note that, in each folder there is one long sequence of single exercise, except exercise 4 which is performed two times in different ways. Those two sequences belongs to same class. Do the following subtasks to pre-analyse data examples and to prepare the training and testing data for next tasks:\n",
    "<br>\n",
    "<br>\n",
    "<p> Read raw data from the files. Prepare and divide each data file to shorter sequences using windowing method. Similar to related article \"MEx: Multi-modal Exercises Dataset for Human Activity Recognition\", use 5 second window and 3 second overlapping between windows, producing several example sequences from one exercise file for classification purposes. Windowing is working so that starting from the beginning of each long exercise sequence, take 5 seconds of data points (from synchronized acceleration data and depth images) based on the time stamps. Next, move the window 2 seconds forward and take another 5 seconds of data. Then continue this until your are at the end of sequence. Each window will consists 500x3 matrix of acceleration data and 5x192 matrix of depth image data.</p>\n",
    "<br>\n",
    "<p> <b>1.1</b> Plot few examples of prepared data for each modalities (accelometer and depth camera). Plot acceleration sensor as multi-dimensional time-series and depth camera data as 2D image. Plot 5 second acceleration sensor and depth image sequences of person 1 and 5 performing exercises 2, 5, and 6. Take the first windowed example from the long exercise sequence. </p>\n",
    "<br>\n",
    "<p> <b>1.2</b> Split the prepared dataset to training and testing datasets so that data of persons 1-7 are used for training and data of persons 8-10 are used for testing. In next tasks, training dataset could be further divided on (multiple) validation data folds to tune the models parameters, when needed.<br>\n",
    "\n",
    "<p> Note: Training set should have 1486 windows and testing set should have 598 windows. In training set, acceleration data will have a window without a pair with depth camera data, that window should be dropped as it doesn't have a pair.<p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Document your work, calculate the indicator statistics of training and testing datasets (number of examples, dimensions of each example) and visualize prepared examples.\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import relevant libraries here\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enter data folder location\n",
    "loc = \"./MEx\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find, read, and compose the measurements\n",
    "from utilities.fun_one import path_to_meta\n",
    "paths_record = Path(loc).glob(\"*/*/*.csv\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for path_record in paths_record:\n",
    "    df = pd.read_csv(path_record, delimiter=\",\", header=None)\n",
    "    meta = path_to_meta(path_record)\n",
    "\n",
    "    if meta[\"sensor\"] == \"acc\":\n",
    "        col_names = [\"time\", \"acc_0\", \"acc_1\", \"acc_2\"]\n",
    "        df.columns = col_names\n",
    "    else:\n",
    "        num_cols = df.shape[1]\n",
    "        col_names = [\"time\", ] + [f\"dc_{i}\" for i in range(num_cols-1)]\n",
    "        df.columns = col_names\n",
    "\n",
    "    meta[\"df\"] = df\n",
    "\n",
    "    records.append(meta)\n",
    "\n",
    "df_records = pd.DataFrame.from_records(records)\n",
    "\n",
    "print(f\"Total records found: {len(df_records)}\")\n",
    "print(\"Dataframe with all records:\")\n",
    "display(df_records.head())\n",
    "print(\"Dataframe with one measurement series:\")\n",
    "display(df_records[\"df\"].iloc[0].head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract 5-second long windows with 2-second shift (3-second overlap)\n",
    "\n",
    "records_windowed = []\n",
    "\n",
    "time_window = 5000.\n",
    "time_offset = 2000.\n",
    "\n",
    "for row_idx, row_data in df_records.iterrows():\n",
    "    df_tmp = row_data[\"df\"]\n",
    "    time_start = np.min(df_tmp[\"time\"].to_numpy())\n",
    "    time_end = np.max(df_tmp[\"time\"].to_numpy())\n",
    "\n",
    "    for window_idx, t0 in enumerate(np.arange(time_start, time_end, time_offset)):\n",
    "        t1 = t0 + time_window\n",
    "        # Handle boundary conditions - skip the measurements from the end shorter than window size\n",
    "        if t1 > time_end:\n",
    "            continue\n",
    "\n",
    "        tmp_data = deepcopy(row_data)\n",
    "        tmp_data[\"window_idx\"] = window_idx\n",
    "        tmp_data[\"df\"] = df_tmp[(df_tmp[\"time\"] >= t0) &\n",
    "                                (df_tmp[\"time\"] < t1)].copy()\n",
    "\n",
    "        records_windowed.append(tmp_data)\n",
    "\n",
    "df_records_windowed = pd.DataFrame.from_records(records_windowed)\n",
    "\n",
    "print(f\"Total windows extracted: {len(df_records_windowed)}\")\n",
    "print(\"Dataframe with all windowed records:\")\n",
    "display(df_records_windowed.head())\n",
    "print(\"Dataframe with one windowed measurement series:\")\n",
    "display(df_records_windowed[\"df\"].iloc[0].head())\n",
    "%store df_records_windowed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Visualize selected samples for both modalities\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import importlib, utilities\n",
    "importlib.reload(utilities)\n",
    "from utilities.fun_one import visualize, visualize_acceleration, visualize_depth, visualize_depth_series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation - Acceleration:\n",
    "%matplotlib inline\n",
    "print(\"Acceleration, Person 1, Exercise 2:\")\n",
    "visualize_acceleration(df_records_windowed,1,2)\n",
    "print(\"Acceleration, Person 1, Exercise 5:\")\n",
    "visualize_acceleration(df_records_windowed,1,5)\n",
    "print(\"Acceleration, Person 1, Exercise 6:\")\n",
    "visualize_acceleration(df_records_windowed,1,6)\n",
    "print(\"Acceleration, Person 5, Exercise 2:\")\n",
    "visualize_acceleration(df_records_windowed,5,2)\n",
    "print(\"Acceleration, Person 5, Exercise 5:\")\n",
    "visualize_acceleration(df_records_windowed,5,5)\n",
    "print(\"Acceleration, Person 5, Exercise 6:\")\n",
    "visualize_acceleration(df_records_windowed,5,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation - Depth:\n",
    "%matplotlib inline\n",
    "print(\"Visualisation Depth Series, First Window:\")\n",
    "print(\"Person 1, Exercise 2:\")\n",
    "visualize_depth_series(df_records_windowed,1,2)\n",
    "print(\"Person 1, Exercise 5:\")\n",
    "visualize_depth_series(df_records_windowed,1,5)\n",
    "print(\"Person 1, Exercise 6:\")\n",
    "visualize_depth_series(df_records_windowed,1,6)\n",
    "print(\"Person 5, Exercise 2:\")\n",
    "visualize_depth_series(df_records_windowed,5,2)\n",
    "print(\"Person 5, Exercise 5:\")\n",
    "visualize_depth_series(df_records_windowed,5,5)\n",
    "print(\"Person 5, Exercise 6:\")\n",
    "visualize_depth_series(df_records_windowed,5,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 2:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,2,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 5:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,5,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 6:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,6,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 2:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,2,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 5:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,5,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 6:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,6,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Split samples based on subject ID into training and testing datasets for futher experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "importlib.reload(utilities.fun_one)\n",
    "from utilities.fun_one import stringify_id\n",
    "# 1.2. Split samples based on subject ID into training and testing datasets for futher experiments\n",
    "print(\"Splitting data into train ( persons 1-7 ) and test set (persons 8-10)\")\n",
    "\n",
    "#Create training and testing set by combining chosen subject data:\n",
    "training_records = df_records_windowed[(df_records_windowed.subject_id == stringify_id(1)) |\n",
    "                                       (df_records_windowed.subject_id == stringify_id(2)) |\n",
    "                                       (df_records_windowed.subject_id == stringify_id(3)) |\n",
    "                                       (df_records_windowed.subject_id == stringify_id(4)) |\n",
    "                                       (df_records_windowed.subject_id == stringify_id(5)) |\n",
    "                                       (df_records_windowed.subject_id == stringify_id(6)) |\n",
    "                                       (df_records_windowed.subject_id == stringify_id(7))]\n",
    "testing_records = df_records_windowed[(df_records_windowed.subject_id == stringify_id(8)) |\n",
    "                                      (df_records_windowed.subject_id == stringify_id(9)) |\n",
    "                                      (df_records_windowed.subject_id == stringify_id(10))]\n",
    "\n",
    "# Drop one row from training set which does not have a pair of sensor readings:\n",
    "training_records = training_records.drop(training_records.index[(training_records.subject_id == stringify_id(2)) &\n",
    "                                                                (training_records.exercise_id == stringify_id(6)) &\n",
    "                                                                (training_records.sensor_code == 'act') &\n",
    "                                                                (training_records.window_idx == 29)])\n",
    "\n",
    "\n",
    "#Save for use in other notebooks:\n",
    "%store training_records\n",
    "%store testing_records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testing_records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
