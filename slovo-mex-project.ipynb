{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-708d46d3f9180abe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Multi-Modal Data Fusion - Project Work: Multi-Modal Physical Exercise Classification\n",
    "\n",
    "\n",
    "In this project, real multi-modal data is studied by utilizing different techniques presented during the course. In addition, there is an optional task to try some different approaches to identify persons from the same dataset. Open MEx dataset from UCI machine learning repository is used. Idea is to apply different techniques to recognize physical exercises from wearable sensors and depth camera, user-independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "Add your information here\n",
    "\n",
    "Name(s):\n",
    "Aleksander Madajczak, Jan Fabian\n",
    "\n",
    "Student number(s):\n",
    "2207367\n",
    "2207371\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32738734cf6f1a4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Description \n",
    "\n",
    "The goal of this project is to develop user-independent pre-processing and classification models to recognize 7 different physical exercises measured by accelerometer (attached to subject's thigh) and depth camera (above the subject facing downwards recording an aerial view). All the exercises were performed subject lying down on the mat. Original dataset have also another acceleration sensor and pressure-sensitive mat, but those two modalities are ommited in this project. There are totally 30 subjects in the original dataset, and in this work subset of 10 person is utilized. Detailed description of the dataset and original data can be access in [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#). We are providing the subset of dataset in Moodle.\n",
    "\n",
    "The project work is divided on following phases:\n",
    "\n",
    "1. Data preparation, exploration, and visualization\n",
    "2. Feature extraction and unimodal fusion for classification\n",
    "3. Feature extraction and feature-level fusion for multimodal classification\n",
    "4. Decision-level fusion for multimodal classification\n",
    "5. Bonus task: Multimodal biometric identification of persons\n",
    "\n",
    "where 1-4 are compulsory (max. 10 points each), and 5 is optional to get bonus points (max. 5+5 points). In each phase, you should visualize and analyse the results and document the work and findings properly by text blocks and figures between the code. <b> Nice looking </b> and <b> informative </b> notebook representing your results and analysis will be part of the grading in addition to actual implementation.\n",
    "\n",
    "The results are validated using confusion matrices and F1 scores. F1 macro score is given as \n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{equation}\n",
    "F1_{macro} = \\frac{1}{N} \\sum_i^N F1_i,\n",
    "\\end{equation}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "where $F1_i = 2  \\frac{precision_i * recall_i}{precision_i + recall_i}$, and $N$ is the number of classes.\n",
    "<br>\n",
    "\n",
    "## Learning goals \n",
    "\n",
    "After the project work, you should  \n",
    "\n",
    "- be able to study real world multi-modal data\n",
    "- be able to apply different data fusion techniques to real-world problem\n",
    "- be able to evaluate the results\n",
    "- be able to analyse the outcome\n",
    "- be able to document your work properly\n",
    "\n",
    "## Relevant lectures\n",
    "\n",
    "Lectures 1-8\n",
    "\n",
    "## Relevant exercises\n",
    "\n",
    "Exercises 0-6\n",
    "\n",
    "## Relevant chapters in course book\n",
    "\n",
    "Chapter 1-14\n",
    "\n",
    "## Additional Material \n",
    "\n",
    "* Original dataset [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#)\n",
    "* Related scientific article [MEx: Multi-modal Exercises Dataset for Human Activity Recognition](https://arxiv.org/pdf/1908.08992.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e096db2d9e8c24e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 1. Data preparation, exploration, and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bacbb8f5ae7a2e4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 1.</b>\n",
    "\n",
    "Download data from the Moodle's Project section. Get yourself familiar with the folder structure and data. You can read the data files using the function given below. Each file consists one exercise type performed by single user. Data are divided on multiple folders. Note that, in each folder there is one long sequence of single exercise, except exercise 4 which is performed two times in different ways. Those two sequences belongs to same class. Do the following subtasks to pre-analyse data examples and to prepare the training and testing data for next tasks:\n",
    "<br>\n",
    "<br> \n",
    "<p> Read raw data from the files. Prepare and divide each data file to shorter sequences using windowing method. Similar to related article \"MEx: Multi-modal Exercises Dataset for Human Activity Recognition\", use 5 second window and 3 second overlapping between windows, producing several example sequences from one exercise file for classification purposes. Windowing is working so that starting from the beginning of each long exercise sequence, take 5 seconds of data points (from synchronized acceleration data and depth images) based on the time stamps. Next, move the window 2 seconds forward and take another 5 seconds of data. Then continue this until your are at the end of sequence. Each window will consists 500x3 matrix of acceleration data and 5x192 matrix of depth image data.</p>\n",
    "<br>  \n",
    "<p> <b>1.1</b> Plot few examples of prepared data for each modalities (accelometer and depth camera). Plot acceleration sensor as multi-dimensional time-series and depth camera data as 2D image. Plot 5 second acceleration sensor and depth image sequences of person 1 and 5 performing exercises 2, 5, and 6. Take the first windowed example from the long exercise sequence. </p>\n",
    "<br>\n",
    "<p> <b>1.2</b> Split the prepared dataset to training and testing datasets so that data of persons 1-7 are used for training and data of persons 8-10 are used for testing. In next tasks, training dataset could be further divided on (multiple) validation data folds to tune the models parameters, when needed.<br>\n",
    "    \n",
    "<p> Note: Training set should have 1486 windows and testing set should have 598 windows. In training set, acceleration data will have a window without a pair with depth camera data, that window should be dropped as it doesn't have a pair.<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Document your work, calculate the indicator statistics of training and testing datasets (number of examples, dimensions of each example) and visualize prepared examples.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries here\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enter data folder location\n",
    "loc = \"./MEx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b805f4284f1480c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def path_to_meta(p):\n",
    "    meta = dict()\n",
    "    meta[\"subject_id\"] = p.parent.stem\n",
    "    meta[\"exercise_id\"] = p.stem.split(\"_\")[-2]\n",
    "    meta[\"trial\"] = int(p.stem.split(\"_\")[-1])\n",
    "    meta[\"sensor_code\"] = p.stem.split(\"_\")[0]\n",
    "    meta[\"sensor\"] = {\"act\": \"acc\", \"dc\": \"dc\"}[meta[\"sensor_code\"]]\n",
    "    return meta\n",
    "\n",
    "# Find, read, and compose the measurements\n",
    "paths_record = Path(loc).glob(\"*/*/*.csv\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for path_record in paths_record:\n",
    "    df = pd.read_csv(path_record, delimiter=\",\", header=None)\n",
    "    meta = path_to_meta(path_record)\n",
    "    \n",
    "    if meta[\"sensor\"] == \"acc\":\n",
    "        col_names = [\"time\", \"acc_0\", \"acc_1\", \"acc_2\"]\n",
    "        df.columns = col_names\n",
    "    else:\n",
    "        num_cols = df.shape[1]\n",
    "        col_names = [\"time\", ] + [f\"dc_{i}\" for i in range(num_cols-1)]\n",
    "        df.columns = col_names\n",
    "\n",
    "    meta[\"df\"] = df\n",
    "    \n",
    "    records.append(meta)\n",
    "\n",
    "df_records = pd.DataFrame.from_records(records)\n",
    "\n",
    "print(f\"Total records found: {len(df_records)}\")\n",
    "print(\"Dataframe with all records:\")\n",
    "display(df_records.head())\n",
    "print(\"Dataframe with one measurement series:\")\n",
    "display(df_records[\"df\"].iloc[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8aebacdeb29d491",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract 5-second long windows with 2-second shift (3-second overlap)\n",
    "\n",
    "records_windowed = []\n",
    "\n",
    "time_window = 5000.\n",
    "time_offset = 2000.\n",
    "    \n",
    "for row_idx, row_data in df_records.iterrows():\n",
    "    df_tmp = row_data[\"df\"]\n",
    "    time_start = np.min(df_tmp[\"time\"].to_numpy())\n",
    "    time_end = np.max(df_tmp[\"time\"].to_numpy())\n",
    "    \n",
    "    for window_idx, t0 in enumerate(np.arange(time_start, time_end, time_offset)):\n",
    "        t1 = t0 + time_window\n",
    "        # Handle boundary conditions - skip the measurements from the end shorter than window size\n",
    "        if t1 > time_end:\n",
    "            continue\n",
    "        \n",
    "        tmp_data = deepcopy(row_data)\n",
    "        tmp_data[\"window_idx\"] = window_idx\n",
    "        tmp_data[\"df\"] = df_tmp[(df_tmp[\"time\"] >= t0) &\n",
    "                                (df_tmp[\"time\"] < t1)].copy()\n",
    "        \n",
    "        records_windowed.append(tmp_data)\n",
    "        \n",
    "df_records_windowed = pd.DataFrame.from_records(records_windowed)\n",
    "\n",
    "print(f\"Total windows extracted: {len(df_records_windowed)}\")\n",
    "print(\"Dataframe with all windowed records:\")\n",
    "display(df_records_windowed.head())\n",
    "print(\"Dataframe with one windowed measurement series:\")\n",
    "display(df_records_windowed[\"df\"].iloc[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1. Visualize selected samples for both modalities\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "### Your code begins here ###\n",
    "def stringify_id(id,fill=2):\n",
    "    ''' Use to change int into the string with zeros prefix ex: 1 -> '01', 0 -> '00' 10 -> '10'. The fill arg determines the minimal number of characters in string (default 2)'''\n",
    "    return str(id).zfill(fill)\n",
    "\n",
    "def filter_dataframe(data,subject_id,exercise_id,sensor_id,window_id=0):\n",
    "    '''\n",
    "    Filter data, get only data with correct subject, exercise and sensor.\n",
    "    If using windowed data the window_id selects which window to return -\n",
    "    for non windowed data use 0.\n",
    "    '''\n",
    "    #First I create the filters:\n",
    "    right_subject = data.subject_id==stringify_id(subject_id) # Only those rows with matching subject id (subject id is saved as '0X' string so must be converted first)\n",
    "    right_exercise = data.exercise_id==stringify_id(exercise_id) # Only those rows with matching exercise id\n",
    "    right_sensor = data.sensor_code==sensor_id #get only depth camera entries\n",
    "    #Then I apply the filters:\n",
    "    data_frame = data[right_subject & right_exercise & right_sensor] # apply filters to the data frame\n",
    "    data_np = data_frame.df.iloc[window_id].to_numpy() # extract to numpy narray\n",
    "    return data_np\n",
    "\n",
    "def visualize_depth_series(data,subject_id,exercise_id,window_id=0):\n",
    "    '''\n",
    "    Visualize depth camera data,\n",
    "    the sensor readings are flattened and presented as a 2d timeseries\n",
    "    '''\n",
    "    depth_np = filter_dataframe(data,subject_id,exercise_id,'dc',window_id)\n",
    "    depth_time = depth_np[:,0] # get timestamps\n",
    "    depth_data = depth_np[:,1:] # get data and remove the first column with timestamps\n",
    "    f = plt.figure(figsize=(10,5))\n",
    "    plt.imshow(depth_data,cmap=plt.get_cmap('gray')) # plot the data with gray color pallet\n",
    "    plt.title(f\"Dc Full, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}\")\n",
    "    plt.yticks((depth_time[::5])/1000)\n",
    "    plt.ylabel(\"time [s]\")\n",
    "    plt.xlabel(\"channels\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_depth(data,subject_id,exercise_id,sample_id=None,window_id=0):\n",
    "    '''\n",
    "    Visualize depth camera data reading as 12x16 image,\n",
    "    which reading is plotted is controlled by sample_id\n",
    "    '''\n",
    "    depth_np = filter_dataframe(data,subject_id,exercise_id,'dc',window_id)\n",
    "    depth_time = depth_np[:,0] # get timestamps\n",
    "    depth_data = depth_np[:,1:] # get data and remove the first column with timestamps\n",
    "    anim = None\n",
    "    if sample_id is not None:\n",
    "        #Plot One Image:\n",
    "        sample_depth_img = depth_data[sample_id].reshape((12,16))\n",
    "        f = plt.figure(figsize=(10,5))\n",
    "        plt.imshow(sample_depth_img,cmap=plt.get_cmap('gray')) # plot the data with gray color pallet\n",
    "        plt.title(f\"Dc, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}, s{sample_id}\")\n",
    "        plt.xticks(list(range(0,16,2))+ [15])\n",
    "        plt.yticks(list(range(0,12,2))+ [11])\n",
    "        #plt.margins(x=0, y=0)\n",
    "        plt.show()\n",
    "    else:\n",
    "        #Plot Image series:\n",
    "        f, ax = plt.subplots()\n",
    "        images = []\n",
    "        for i,img in enumerate(depth_data):\n",
    "            image = img.reshape(12,16)\n",
    "            im = ax.imshow(image, cmap=plt.get_cmap('gray'), animated=True) # plot the data with gray color pallet\n",
    "            title_text = f\"Dc, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}, s{str(i)}, t{(depth_time[i]/1000):.0f}[s]\"\n",
    "            title = ax.text(0.5,1.05, title_text,\n",
    "                    size=plt.rcParams[\"axes.titlesize\"],\n",
    "                    ha=\"center\", transform=ax.transAxes)\n",
    "            if i == 0:\n",
    "               ax.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "               #continue\n",
    "            images.append([im,title])\n",
    "\n",
    "        plt.xticks(list(range(0,16,2))+ [15])\n",
    "        plt.yticks(list(range(0,12,2))+ [11])\n",
    "        anim = animation.ArtistAnimation(f, images, interval=1000, blit=True, repeat_delay=1000)\n",
    "        plt.show()\n",
    "        return anim\n",
    "\n",
    "def visualize_acceleration(data,subject_id,exercise_id,window_id=0):\n",
    "    '''\n",
    "    Visualize acceleration readings as as 3 timeseries.\n",
    "    '''\n",
    "    accele_np = filter_dataframe(data,subject_id,exercise_id,'act',window_id)\n",
    "    accele_time = accele_np[:,0] # get timestamps 100Hz\n",
    "    accele_d1 = accele_np[:,1] # get first timeseries x\n",
    "    accele_d2 = accele_np[:,2] # get second timeseries y\n",
    "    accele_d3 = accele_np[:,3] # get third timeseries z\n",
    "    f = plt.figure(figsize=(10,5))\n",
    "    plt.plot(accele_time,accele_d1,c='blue',label='x') # plot first timeseries\n",
    "    plt.plot(accele_time,accele_d2,c='orange',label='y') # plot second timeseries\n",
    "    plt.plot(accele_time,accele_d3,c='red',label='z') # plot third timeseries\n",
    "    plt.title(f\"Act, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}\")\n",
    "    plt.xticks(accele_time[::1000],(accele_time[::1000])*0.01)\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    plt.ylabel(\"acceleration [g]\") # g - of earths acceleration\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation - Acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(\"Acceleration, Person 1, Exercise 2:\")\n",
    "visualize_acceleration(df_records_windowed,1,2)\n",
    "print(\"Acceleration, Person 1, Exercise 5:\")\n",
    "visualize_acceleration(df_records_windowed,1,5)\n",
    "print(\"Acceleration, Person 1, Exercise 6:\")\n",
    "visualize_acceleration(df_records_windowed,1,6)\n",
    "print(\"Acceleration, Person 5, Exercise 2:\")\n",
    "visualize_acceleration(df_records_windowed,5,2)\n",
    "print(\"Acceleration, Person 5, Exercise 5:\")\n",
    "visualize_acceleration(df_records_windowed,5,5)\n",
    "print(\"Acceleration, Person 5, Exercise 6:\")\n",
    "visualize_acceleration(df_records_windowed,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation - Depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(\"Visualisation Depth Series, First Window:\")\n",
    "print(\"Person 1, Exercise 2:\")\n",
    "visualize_depth_series(df_records_windowed,1,2)\n",
    "print(\"Person 1, Exercise 5:\")\n",
    "visualize_depth_series(df_records_windowed,1,5)\n",
    "print(\"Person 1, Exercise 6:\")\n",
    "visualize_depth_series(df_records_windowed,1,6)\n",
    "print(\"Person 5, Exercise 2:\")\n",
    "visualize_depth_series(df_records_windowed,5,2)\n",
    "print(\"Person 5, Exercise 5:\")\n",
    "visualize_depth_series(df_records_windowed,5,5)\n",
    "print(\"Person 5, Exercise 6:\")\n",
    "visualize_depth_series(df_records_windowed,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 2:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,2,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 5:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,5,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 6:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,6,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 2:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,2,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 5:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,5,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 6:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,6,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# 1.2. Split samples based on subject ID into training and testing datasets for futher experiments\n",
    "print(\"Splitting data into train ( persons 1-7 ) and test set (persons 8-10)\")\n",
    "### Your code begins here ###\n",
    "training_records = df_records_windowed[(df_records_windowed.subject_id==stringify_id(1)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(2)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(3)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(4)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(5)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(6)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(7)) ]\n",
    "testing_records  = df_records_windowed[(df_records_windowed.subject_id==stringify_id(8)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(9)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(10))]\n",
    "#Drop one row from training set which does not have a pair of sensor readings:\n",
    "training_records = training_records.drop(training_records.index[(training_records.subject_id==stringify_id(2) ) &\n",
    "                                             (training_records.exercise_id==stringify_id(6) ) &\n",
    "                                             (training_records.sensor_code=='act' )  &\n",
    "                                             (training_records.window_idx==29 )])\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8dd9cccbd7bb483c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Feature extraction and fusion for unimodal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-583b0e4a5d64720f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 2.</b>\n",
    "\n",
    "Use the training dataset prepared in task 1. to build models based on the combination of principal component analysis (PCA), linear discriminant analysis (LDA), and nearest neighbour (NN) classifier for each modality separately and evaluate the model on test dataset. Do the subtasks given as\n",
    "<br>\n",
    "<br>\n",
    "<p> <b>2.1</b> Calculate PCA and LDA transformations to reduce the dimensionality of accelerometer data (e.g., using scikit-learn implementations). Before transformations downsample data from 100 Hz to 25 Hz (using scipy.signal.resample) to get 125x3 matrix of data for each 5 sec window. You should also standardize the values to zero mean and unit variance before the transformations. Using training dataset, fit PCA with 5-dimensional subspace (i.e., choosing the 5 largest principal components) and fit LDA with 5-dimensional subspace. Transform both train and test examples to this low-dimensional feature representation. Concatenate each sequence to single vector size of 3x(5+5). Perform the fusion of PCA and LDA similar manner as presented in Lecture 3 (pages 24-25) using NN method. Evaluate the performance on testset. Show confusion matrix and F1 scores of the results. </p>\n",
    "<br>\n",
    "<p> <b>2.2</b> Use PCA and LDA transformations to reduce the dimensionality of depth images. You should also standardize the values to zero mean and unit variance before the transformations. Fit PCA and LDA for all training images (12x16, 192-dimensional in vectorized form) by choosing 5-dimensional subspace for both PCA and LDA. Transform both train and test examples to this low-dimensional feature representation. Concatenate each sequence to single vector size of 5x1x(5+5). Similar to task 2.1, do the PCA and LDA fusion using NN and evaluate the performance on testset. Show confusion matrix and F1 scores of the results. </p>\n",
    "<br> \n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 2.1-2.2.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Task 2 imports'''\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def acccelerometer_resample(data,n_samples=125):\n",
    "    return data[data.sensor_code=='act'].df.apply(\n",
    "            lambda x: pd.DataFrame().assign(\n",
    "                act_0=signal.resample(x.acc_0,n_samples),\n",
    "                act_1=signal.resample(x.acc_1,n_samples),\n",
    "                act_2=signal.resample(x.acc_2,n_samples)))\n",
    "\n",
    "class Standardizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data,) -> None:\n",
    "        '''Concatenate all dataframes to one to calculate mean and std'''\n",
    "        samples_concatenated = pd.concat(data['df'].values, ignore_index=True)\n",
    "        if 'time' in samples_concatenated:\n",
    "            samples_concatenated = samples_concatenated.drop('time', axis=1)\n",
    "        self.mean = np.mean(samples_concatenated, axis=0)\n",
    "        self.std = np.std(samples_concatenated, axis=0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        standardized_data = deepcopy(data)\n",
    "\n",
    "        for index, row in standardized_data.iterrows():\n",
    "            df = row['df']\n",
    "            if 'time' in df:\n",
    "                df = df.drop('time', axis=1)\n",
    "            df_standardized = (df - self.mean) / self.std\n",
    "            standardized_data.at[index,'df'] = df_standardized\n",
    "        return standardized_data\n",
    "\n",
    "    def display_mean_and_std(self) -> None:\n",
    "        display(self.mean)\n",
    "        display(self.std)\n",
    "\n",
    "class PcaActApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.pca = [PCA(n_components), PCA(n_components), PCA(n_components)]\n",
    "\n",
    "    def fit(self, data) -> None:\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        self.pca[0].fit(x[0])\n",
    "        self.pca[1].fit(x[1])\n",
    "        self.pca[2].fit(x[2])\n",
    "\n",
    "    def transform(self, data):\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        return self.pca[0].transform(x[0]), self.pca[1].transform(x[1]), self.pca[2].transform(x[2])\n",
    "\n",
    "    def get_pca(self):\n",
    "        return self.pca\n",
    "\n",
    "class LdaActApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.lda = LDA(n_components = n_components), LDA(n_components = n_components), LDA(n_components = n_components)\n",
    "\n",
    "    def fit(self, data, labels) -> None:\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        self.lda[0].fit(x[0],labels)\n",
    "        self.lda[1].fit(x[1],labels)\n",
    "        self.lda[2].fit(x[2],labels)\n",
    "\n",
    "    def transform(self, data):\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        return self.lda[0].transform(x[0]), self.lda[1].transform(x[1]), self.lda[2].transform(x[2])\n",
    "\n",
    "    def get_lda(self):\n",
    "        return self.lda\n",
    "\n",
    "def act_fusion(act_pca_train, act_lda_train, act_pca_test, act_lda_test, train_labels):\n",
    "    cobined  = np.dstack((np.concatenate((act_pca_train[0],act_lda_train[0]),axis=1),\n",
    "                     np.concatenate((act_pca_train[1],act_lda_train[1]),axis=1),\n",
    "                     np.concatenate((act_pca_train[2],act_lda_train[2]),axis=1)))\n",
    "\n",
    "    test_cobined  = np.dstack((np.concatenate((act_pca_test[0],act_lda_test[0]),axis=1),\n",
    "                          np.concatenate((act_pca_test[1],act_lda_test[1]),axis=1),\n",
    "                          np.concatenate((act_pca_test[2],act_lda_test[2]),axis=1)))\n",
    "    # Fusion\n",
    "    labels = np.zeros((test_cobined.shape[0],4))\n",
    "    for i,sample in enumerate(test_cobined): # chose one data point to classify # (N, K,  xyz )\n",
    "        d = np.zeros((cobined.shape[0],3))\n",
    "        D = np.zeros((cobined.shape[0],3))\n",
    "        for n in range(0,cobined.shape[0]): # Iterate over all samples\n",
    "            d[n] = np.sum( [ (sample[k] - cobined[n][k])**2 for k in range(0,5) ],axis=0)\n",
    "            D[n] = np.sum( [ (sample[k] - cobined[n][k])**2 for k in range(5,10)],axis=0)\n",
    "\n",
    "        d_scaled = np.array([(dn - np.min(d))/ (np.max(d) - np.min(d)) for dn in d])\n",
    "        D_scaled = np.array([(Dn - np.min(D))/ (np.max(D) - np.min(D)) for Dn in D])\n",
    "        F = 0.5 * (d_scaled + D_scaled)\n",
    "        n_star = np.argmin(F,axis=0)\n",
    "        label = train_labels.to_numpy()[n_star] # array with exercise_id\n",
    "        #Combined\n",
    "        F_c = np.sum(F,axis=1)\n",
    "        n_star_c = np.argmin(F_c,axis=0)\n",
    "        c_label =  train_labels.to_numpy()[n_star_c]\n",
    "        labels[i] = np.concatenate((label.flatten(), np.array([c_label])))\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.1\n",
    "### Your code begins here ###\n",
    "# Resample data:\n",
    "resample_samples = 125\n",
    "act_train = pd.DataFrame()\n",
    "act_train['df'] = acccelerometer_resample(training_records,resample_samples)\n",
    "train_labels = training_records[training_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "act_test = pd.DataFrame()\n",
    "act_test['df'] = acccelerometer_resample(testing_records,resample_samples)\n",
    "test_labels = testing_records[testing_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "\n",
    "#Standardize the Data:\n",
    "act_s = Standardizer()\n",
    "act_s.fit(act_train)\n",
    "act_train['df'] = act_s.transform(act_train)\n",
    "act_test['df'] = act_s.transform(act_test)\n",
    "\n",
    "#PCA\n",
    "n_components = 5\n",
    "pca = PcaActApplier(n_components)\n",
    "pca.fit(act_train['df'])\n",
    "act_pca_train = pca.transform(act_train['df'])\n",
    "act_pca_test = pca.transform(act_test['df'])\n",
    "\n",
    "#act_0 = act_train['df'].apply(lambda x: x.act_0.T)\n",
    "#act_1 = act_train['df'].apply(lambda x: x.act_1.T)\n",
    "#act_2 = act_train['df'].apply(lambda x: x.act_2.T)\n",
    "#t_act_0 = act_test['df'].apply(lambda x: x.act_0.T)\n",
    "#t_act_1 = act_test['df'].apply(lambda x: x.act_1.T)\n",
    "#t_act_2 = act_test['df'].apply(lambda x: x.act_2.T)\n",
    "#pca_0 = PCA(5)\n",
    "#pca_1 = PCA(5)\n",
    "#pca_2 = PCA(5)\n",
    "#pca_0.fit(act_0)\n",
    "#pca_1.fit(act_1)\n",
    "#pca_2.fit(act_2)\n",
    "#p_0 = pca_0.transform(act_0)\n",
    "#p_1 = pca_1.transform(act_1)\n",
    "#p_2 = pca_2.transform(act_2)\n",
    "#t_p_0 = pca_0.transform(t_act_0)\n",
    "#t_p_1 = pca_1.transform(t_act_1)\n",
    "#t_p_2 = pca_2.transform(t_act_2)\n",
    "\n",
    "#LDA:\n",
    "n_components = 5\n",
    "lda = LdaActApplier(n_components)\n",
    "lda.fit(act_train['df'],train_labels)\n",
    "act_lda_train = lda.transform(act_train['df'])\n",
    "act_lda_test = lda.transform(act_test['df'])\n",
    "\n",
    "#lda_0 = LDA(n_components = 5)\n",
    "#lda_1 = LDA(n_components = 5)\n",
    "#lda_2 = LDA(n_components = 5)\n",
    "#lda_0.fit(act_0,y)\n",
    "#lda_1.fit(act_1,y)\n",
    "#lda_2.fit(act_2,y)\n",
    "#l_0 = lda_0.transform(act_0)\n",
    "#l_1 = lda_1.transform(act_1)\n",
    "#l_2 = lda_2.transform(act_2)\n",
    "#t_l_0 = lda_0.transform(t_act_0)\n",
    "#t_l_1 = lda_1.transform(t_act_1)\n",
    "#t_l_2 = lda_2.transform(t_act_2)\n",
    "#\n",
    "#cobined  = np.stack((np.concatenate((p_0,l_0),axis=1),\n",
    "#                     np.concatenate((p_1,l_1),axis=1),\n",
    "#                     np.concatenate((p_2,l_2),axis=1)))\n",
    "#test_cobined  = np.stack((np.concatenate((t_p_0,t_l_0),axis=1),\n",
    "#                          np.concatenate((t_p_1,t_l_1),axis=1),\n",
    "#                          np.concatenate((t_p_2,t_l_2),axis=1)))\n",
    "# Fusion:\n",
    "train_pred_labels = act_fusion(act_pca_train,act_lda_train,act_pca_train,act_lda_train,train_labels)\n",
    "test_pred_labels = act_fusion(act_pca_train,act_lda_train,act_pca_test,act_lda_test,train_labels)\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation:\n",
    "from sklearn import metrics\n",
    "train_matrix = metrics.confusion_matrix(np.array(train_labels), train_pred_labels[:,3])\n",
    "train_f1 = metrics.f1_score(np.array(train_labels),train_pred_labels[:,3],average='macro')\n",
    "test_matrix = metrics.confusion_matrix(np.array(test_labels), test_pred_labels[:,3])\n",
    "test_f1 = metrics.f1_score(np.array(test_labels),test_pred_labels[:,3],average='macro')\n",
    "print(\"Training data F1 score = \", train_f1)\n",
    "print(\"Testing data F1 score = \", test_f1)\n",
    "#Plot the matrixes:\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].title.set_text(\"Training data confusion matrix:\")\n",
    "ev_mat_disp = metrics.ConfusionMatrixDisplay(train_matrix)\n",
    "ev_mat_disp.plot(ax=ax[0])\n",
    "ax[1].title.set_text(\"Testing data confusion matrix:\")\n",
    "ts_mat_disp = metrics.ConfusionMatrixDisplay(test_matrix)\n",
    "ts_mat_disp.plot(ax=ax[1])\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fusion - based on act_0\n",
    "ex = test_cobined[0][0] # chose one data point to classify # ( xyz, N, K )\n",
    "d = np.zeros(cobined.shape[1])\n",
    "D = np.zeros(cobined.shape[1])\n",
    "for n in range(0,cobined.shape[1]): # Iterate over all samples\n",
    "    d[n] = np.sum( [ (ex[k] - cobined[0][n][k])**2 for k in range(0,5)] )\n",
    "    D[n] = np.sum( [ (ex[k] - cobined[0][n][k])**2 for k in range(5,10)])\n",
    "\n",
    "d_scaled = [(d - np.min(d))/ (np.max(d) - np.min(d)) for dn in d]\n",
    "D_scaled = [(D - np.min(D))/ (np.max(D) - np.min(D)) for Dn in D]\n",
    "F = 0.5 * (d + D)\n",
    "n_star = np.argmin(F) #629\n",
    "y.to_numpy()[n_star] # array with exercise_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Task 2 imports'''\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, ConfusionMatrixDisplay\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Task 2.2 helpers'''\n",
    "#helper methods\n",
    "def concat_pca_lda(pca_data, lda_data):\n",
    "    concatenated_data = deepcopy(pca_data)\n",
    "    for index, sample in pca_data.iterrows():\n",
    "        df_concatenated = pd.concat([pca_data['df'].loc[index], lda_data['df'].loc[index]], axis=1, ignore_index=True)\n",
    "        concatenated_data.at[index,'df'] = df_concatenated\n",
    "    return concatenated_data\n",
    "\n",
    "#helper classes\n",
    "class Standardizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data) -> None:\n",
    "        '''Concatenate all dataframes to one to calculate mean and std'''\n",
    "        samples_concatenated = pd.concat(data['df'].values, ignore_index=True)\n",
    "        if 'time' in samples_concatenated:\n",
    "            samples_concatenated = samples_concatenated.drop('time', axis=1)\n",
    "        self.mean = np.mean(samples_concatenated, axis=0)\n",
    "        self.std = np.std(samples_concatenated, axis=0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        standardized_data = deepcopy(data)\n",
    "\n",
    "        for index, row in standardized_data.iterrows():\n",
    "            df = row['df']\n",
    "            if 'time' in df:\n",
    "                df = df.drop('time', axis=1)\n",
    "            df_standardized = (df - self.mean) / self.std\n",
    "            standardized_data.at[index,'df'] = df_standardized\n",
    "        return standardized_data\n",
    "\n",
    "    def display_mean_and_std(self) -> None:\n",
    "        display(self.mean)\n",
    "        display(self.std)\n",
    "\n",
    "class PcaApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.pca = PCA(n_components)\n",
    "\n",
    "    def fit(self, data) -> None:\n",
    "        '''Concatenate all dataframes to one'''\n",
    "        samples_concatenated = pd.concat(data['df'].values, ignore_index=True)\n",
    "        self.pca.fit(samples_concatenated)\n",
    "\n",
    "    def transform(self, data):\n",
    "        pca_transformed_data = deepcopy(data)\n",
    "        for index, row in pca_transformed_data.iterrows():\n",
    "            df = row['df']\n",
    "            df_pca_transformed = self.pca.transform(df)\n",
    "            pca_transformed_data.at[index,'df'] = pd.DataFrame(df_pca_transformed)\n",
    "        return pca_transformed_data\n",
    "\n",
    "    def get_pca(self):\n",
    "        return self.pca\n",
    "\n",
    "class LdaApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.lda = LDA(n_components = n_components)\n",
    "\n",
    "    def fit(self, data) -> None:\n",
    "        '''Concatenate all dataframes to one'''\n",
    "        samples_concatenated = pd.concat(data['df'].values, ignore_index=True)\n",
    "        labels = []\n",
    "        time_window_length = len(data['df'].values[0])\n",
    "        for index, value in data['exercise_id'].values:\n",
    "            for i in range(0, time_window_length):\n",
    "                labels = np.append(labels,value)\n",
    "        self.lda.fit(samples_concatenated, labels)\n",
    "\n",
    "    def transform(self, data):\n",
    "        lda_transformed_data = deepcopy(data)\n",
    "        for index, row in lda_transformed_data.iterrows():\n",
    "            df = row['df']\n",
    "            df_lda_transformed = self.lda.transform(df)\n",
    "            lda_transformed_data.at[index,'df'] = pd.DataFrame(df_lda_transformed)\n",
    "        return lda_transformed_data\n",
    "\n",
    "    def get_lda(self):\n",
    "        return self.lda\n",
    "\n",
    "def classifyNN(train_data, test_data):\n",
    "    pca_range = range(0,5)\n",
    "    lda_range = range(5,10)\n",
    "\n",
    "    estimated_test_data_classes = pd.DataFrame([], columns=['test_item_index', 'real_label', 'estimated_label'])\n",
    "\n",
    "    for test_index, test_item in test_data.iterrows():\n",
    "        pca_test = test_item['df'].iloc[:,pca_range].values\n",
    "        lda_test = test_item['df'].iloc[:,lda_range].values\n",
    "\n",
    "        '''Calculate distances'''\n",
    "        distances = pd.DataFrame([], columns=['train_item_index', 'dn_pca', 'Dn_lda'])\n",
    "        for train_index, train_item in train_data.iterrows():\n",
    "            pca_train = train_item['df'].iloc[:,pca_range].values\n",
    "            lda_train = train_item['df'].iloc[:,lda_range].values\n",
    "            dn_pca = np.sum(np.sum((pca_test - pca_train)**2, axis=1)) #TODO idk if this is good\n",
    "            Dn_lda = np.sum(np.sum((lda_test - lda_train)**2, axis=1))\n",
    "\n",
    "            new_distance = pd.DataFrame({'train_item_index': train_index,'dn_pca': [dn_pca], 'Dn_lda': [Dn_lda]})\n",
    "            distances = pd.concat([distances, new_distance], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "        min_dn_pca = np.min(distances['dn_pca'])\n",
    "        max_dn_pca = np.max(distances['dn_pca'])\n",
    "        min_Dn_lda = np.min(distances['Dn_lda'])\n",
    "        max_Dn_lda = np.max(distances['Dn_lda'])\n",
    "\n",
    "        distances['scaled_dn_pca'] = (distances['dn_pca'] - min_dn_pca)/(max_dn_pca - min_dn_pca)\n",
    "        distances['scaled_Dn_lda'] = (distances['Dn_lda'] - min_Dn_lda)/(max_Dn_lda - min_Dn_lda)\n",
    "\n",
    "        distances['fused_distances'] = 0.5*(distances['scaled_dn_pca']+distances['scaled_Dn_lda'])\n",
    "\n",
    "        real_label = test_item['exercise_id']\n",
    "        estimated_label = train_data['exercise_id'].loc[distances['train_item_index'].iloc[np.argmin(distances['fused_distances'])]]\n",
    "\n",
    "        new_estimation = pd.DataFrame([{'test_item_index': test_index,'real_label': real_label, 'estimated_label': estimated_label}])\n",
    "        estimated_test_data_classes = pd.concat([estimated_test_data_classes, new_estimation], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    return estimated_test_data_classes\n",
    "\n",
    "def display_results(data):\n",
    "    conf_matrix = confusion_matrix(data['real_label'], data['estimated_label'], labels=['01','02','03','04','05','06','07'])\n",
    "    f1 = f1_score(data['real_label'], data['estimated_label'], average='macro')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=['01','02','03','04','05','06','07'])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    print(\"F1 score is: \", f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.2\n",
    "### Your code begins here ###\n",
    "'''Work only with rows with dc'''\n",
    "dc_training_records = training_records[training_records['sensor'] == 'dc']\n",
    "dc_testing_records = testing_records[testing_records['sensor'] == 'dc']\n",
    "\n",
    "'''Standardize the dc data'''\n",
    "standardizer = Standardizer()\n",
    "standardizer.fit(dc_training_records)\n",
    "standardized_dc_training_records = standardizer.transform(dc_training_records)\n",
    "standardized_dc_testing_records = standardizer.transform(dc_testing_records)\n",
    "\n",
    "'''Initialize PCA and LDA for acc sensor'''\n",
    "reduced_dimensions = 5\n",
    "\n",
    "'''Fit and transform PCA'''\n",
    "#PCA\n",
    "pca_applier = PcaApplier(reduced_dimensions)\n",
    "pca_applier.fit(standardized_dc_training_records)\n",
    "pca_dc_training_records = pca_applier.transform(standardized_dc_training_records)\n",
    "pca_dc_testing_records = pca_applier.transform(standardized_dc_testing_records)\n",
    "\n",
    "#LDA\n",
    "lda_applier = LdaApplier(reduced_dimensions)\n",
    "lda_applier.fit(standardized_dc_training_records)\n",
    "lda_dc_training_records = lda_applier.transform(standardized_dc_training_records)\n",
    "lda_dc_testing_records = lda_applier.transform(standardized_dc_testing_records)\n",
    "\n",
    "#Concatenate\n",
    "concat_dc_training_records = concat_pca_lda(pca_dc_training_records, lda_dc_training_records)\n",
    "concat_dc_testing_records = concat_pca_lda(pca_dc_testing_records, lda_dc_testing_records)\n",
    "\n",
    "# display(pca_dc_training_records['df'].iloc[0].shape)\n",
    "# display(pca_dc_testing_records['df'].iloc[0].shape)\n",
    "# display(lda_dc_training_records['df'].iloc[0].shape)\n",
    "# display(lda_dc_testing_records['df'].iloc[0].shape)\n",
    "# display(concat_dc_training_records['df'].iloc[0].shape)\n",
    "# display(concat_dc_testing_records['df'].iloc[0].shape)\n",
    "\n",
    "'''Classify test samples'''\n",
    "est_classes = classifyNN(train_data = concat_dc_training_records, test_data=concat_dc_testing_records)\n",
    "\n",
    "display_results(est_classes)\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Feature extraction and feature-level fusion for multimodal classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61ec26decd69ed9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 3.</b>\n",
    "\n",
    "Prepare new feature sets for each modality and combine them to single feature representation. Compare two classifiers from scikit-learn. Train classifiers using joint feature presentation. Evaluate and compare the result using testing dataset. Do the subtasks given as\n",
    "<br>   \n",
    "<br> \n",
    "<p> <b>3.1</b> Similar to task 2.1, calculate PCA for accelerometer, but choose now the 10 largest principal components as 10-dim feature vector for each window. In addition, for each window calculate mean and standard deviation of each three acc channels as statistical features, resulting 6-dimensional vector. Combine these to 36-dimensional final feature vector.</p>\n",
    "<br>  \n",
    "<p> <b>3.2</b> Similar to task 2.2, calculate the PCA for depth images using same setup, but now choose the 10 largest principal components as feature vector. Concatenate the image sequence forming 50-dimensional feature vector from each windowed example.</p>\n",
    "<br> \n",
    "<p> <b>3.3</b> Form a joint feature presentation of features extracted in 3.1 and 3.2, resulting 86-dimensional feature vector for each example. Normalize data between 0-1 using the training dataset. Use support vector machine (SVM) with RBF-kernel and Gaussian naiveBayes classifier (use default parameter values for both classifiers). Train the classifiers and evaluate and compare classifiers on testset using confusion matrices and F1 scores.</p>\n",
    "<br> \n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 3.1-3.3.\n",
    "    \n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.1\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cedfc7478c461525",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 4. Decision-level fusion for multimodal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21056ef018219662",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 4.</b>\n",
    "\n",
    "Use features calculated for each modality in task 3. Choose base classifier for each modality from scikit-learn. Train classifiers for each modality feature presentations separately and combine the outputs in decision level. Evaluate and compare the result on testing dataset. Do the subtasks given as\n",
    "<br>\n",
    "<br> \n",
    "<p> <b>4.1</b> Use base classifiers of support vector machine (SVM) with RBF-kernel and AdaBoost classifier (with random_state=0). \n",
    "Normalize data between 0-1 using the training dataset. Train the base classifiers by tuning the model parameters (<i>C</i> parameter and RBF-kernel <i>gamma</i> in SVM as well as <i>n_estimators</i> and <i>learning_rate</i> in Adaboost) using 10-fold cross-validation on training dataset to find optimal set of parameters (hint: use GridSearchCV from scikit-learn). For grid search use the following values $C = [0.1, 1.0, 10.0, 100.0]$, $gamma=[0.1, 0.25, 0.5, 0.75, 1.0, 2.0]$, $n\\_estimators = [50, 100, 500, 1000]$, and $learning\\_rate = [0.1, 0.25, 0.5, 0.75,1.0]$. Choose the best parameters and train the classifiers for each modality on whole training dataset. Is there a possibility that classifiers will overfit to training data using this parameter selection strategy? If so, why? </p>\n",
    "<br>\n",
    "<p> <b>4.2</b> Predict probabilistic outputs of each trained classifier for both modalities using the test set. </p>\n",
    "<br>\n",
    "<p> <b>4.3</b> Combine the probabilistic outputs of different modalities by fixed classification rules: max, min, prod, and sum. Evaluate, compare, and analyse the final combined results using confusion matrices and F1 scores. Show results for each base classifier combinations (i.e., $SVM_{acc}+SVM_{depth}$, $AdaBoost_{acc}+AdaBoost_{depth}$, $SVM_{acc}+AdaBoost_{depth}$, $AdaBoost_{acc}+SVM_{depth}$)</p>\n",
    "<br>\n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 4.1-4.3.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4f13fd6c3cbb70d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 5. Bonus task: Multimodal biometric identification of persons (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e082ae9d74f6755a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 5.</b>\n",
    "\n",
    "Can you build a classifier that recognizes the person who is performing the exercise? Use same 10 person dataset and split it so that first 25% of each long exercise sequence is used for training and rest 75% of each sequence is used for testing the classifier. Use same 5 second windowing with 3 seconds overlap to prepare the examples. Note that, now the person identity is the class label instead of exercise type. Max. 10 points are given but you can earn points from partial solution, as well.\n",
    "<br> \n",
    "<br> \n",
    "<p> <b>5.1</b> Build a classifier to identify persons based on the features and one of the models given in task 4 (max. 5 points).</p>\n",
    "<br> \n",
    "<p> <b>5.2</b> Can you build your own solution (using new features, new classification model or different fusion approaches) to beat the approach in Task 5.1 ? (max. 5 points) </p>\n",
    "<br>  \n",
    "Document your work. Evaluate and compare the results using confusion matrix and F1 score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
