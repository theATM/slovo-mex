{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Feature extraction and feature-level fusion for multimodal classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='task3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 3.</b>\n",
    "\n",
    "Prepare new feature sets for each modality and combine them to single feature representation. Compare two classifiers from scikit-learn. Train classifiers using joint feature presentation. Evaluate and compare the result using testing dataset. Do the subtasks given as\n",
    "<br>\n",
    "<br>\n",
    "<p> <b>3.1</b> Similar to task 2.1, calculate PCA for accelerometer, but choose now the 10 largest principal components as 10-dim feature vector for each window. In addition, for each window calculate mean and standard deviation of each three acc channels as statistical features, resulting 6-dimensional vector. Combine these to 36-dimensional final feature vector.</p>\n",
    "<br>\n",
    "<p> <b>3.2</b> Similar to task 2.2, calculate the PCA for depth images using same setup, but now choose the 10 largest principal components as feature vector. Concatenate the image sequence forming 50-dimensional feature vector from each windowed example.</p>\n",
    "<br>\n",
    "<p> <b>3.3</b> Form a joint feature presentation of features extracted in 3.1 and 3.2, resulting 86-dimensional feature vector for each example. Normalize data between 0-1 using the training dataset. Use support vector machine (SVM) with RBF-kernel and Gaussian naiveBayes classifier (use default parameter values for both classifiers). Train the classifiers and evaluate and compare classifiers on testset using confusion matrices and F1 scores.</p>\n",
    "<br>\n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 3.1-3.3.\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run task one first to get training and testing records (Task 1.2)\n",
    "If you have run task one yourself before you can skip following cell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./slovo_one.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get the training data form the notebook slovo_one:\n",
    "%store -r training_records\n",
    "training_records = training_records\n",
    "%store -r testing_records\n",
    "testing_records = testing_records"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Task 3 imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import importlib, utilities.fun_three, utilities.fun_two, utilities.fun_one\n",
    "importlib.reload(utilities.fun_three)\n",
    "importlib.reload(utilities.fun_two)\n",
    "importlib.reload(utilities.fun_one)\n",
    "from utilities.fun_three import reshape_dataframe, Normalizer, merge_dataframes\n",
    "from utilities.fun_two import *\n",
    "from utilities.fun_one import visualize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3.1 Prepare the PCA with 10 components and channel means and stds for the accelerometer data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Firstly resample the data\n",
    "(Similarity to the task 2.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Your code begins here ###\n",
    "# Resample data:\n",
    "start_time = time.time()\n",
    "# number of samples to be left in a window\n",
    "resample_samples = 125\n",
    "# Resample training data using signal's resample:\n",
    "act_train = pd.DataFrame()\n",
    "act_train['df'] = acccelerometer_resample(training_records,resample_samples)\n",
    "train_labels = training_records[training_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "# Resample testing data:\n",
    "act_test = pd.DataFrame()\n",
    "act_test['df'] = acccelerometer_resample(testing_records,resample_samples)\n",
    "#Get Labels:\n",
    "test_labels = testing_records[testing_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Secondly perform the data standardization\n",
    "(Similarity to the task 2.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Standardize the Data:\n",
    "start_time = time.time()\n",
    "# Create a Standardizer:\n",
    "act_s = Standardizer()\n",
    "# Calculate mean and std on training set:\n",
    "act_s.fit(act_train)\n",
    "# Using mean and std standardize training and test sets:\n",
    "act_train['df'] = act_s.transform(act_train)\n",
    "act_test['df'] = act_s.transform(act_test)\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Thirdly calculate the PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA:\n",
    "start_time = time.time()\n",
    "# choose 10 principal components:\n",
    "n_components = 10\n",
    "# Create Pca decorator class object:\n",
    "pca = PcaActApplier(n_components)\n",
    "# Fit the PCA with the training data\n",
    "pca.fit(act_train['df'])\n",
    "# Apply PCA on both training and testing data:\n",
    "act_pca_train = pca.transform(act_train['df'])\n",
    "act_pca_test = pca.transform(act_test['df'])\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenate accelerometer data features\n",
    "We have 10 PCA components * 3 channels + mean and std form each channel = vector of 36 values per row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Create training accelerometer features np array:\n",
    "act_features = np.concatenate((act_pca_train[0],act_pca_train[1],act_pca_train[2],\n",
    "                               np.mean(act_pca_train[0],axis=1).reshape(-1,1), np.std(act_pca_train[0],axis=1).reshape(-1,1),\n",
    "                               np.mean(act_pca_train[1],axis=1).reshape(-1,1), np.std(act_pca_train[1],axis=1).reshape(-1,1),\n",
    "                               np.mean(act_pca_train[2],axis=1).reshape(-1,1), np.std(act_pca_train[2],axis=1).reshape(-1,1),\n",
    "                              ),axis=1)\n",
    "# Create testing accelerometer features np array:\n",
    "act_test_features = np.concatenate((act_pca_test[0],act_pca_test[1],act_pca_test[2],\n",
    "                                    np.mean(act_pca_test[0],axis=1).reshape(-1,1), np.std(act_pca_test[0],axis=1).reshape(-1,1),\n",
    "                                    np.mean(act_pca_test[1],axis=1).reshape(-1,1), np.std(act_pca_test[1],axis=1).reshape(-1,1),\n",
    "                                    np.mean(act_pca_test[2],axis=1).reshape(-1,1), np.std(act_pca_test[2],axis=1).reshape(-1,1),\n",
    "                                   ),axis=1)\n",
    "\n",
    "# Create the dataframes from the np arrays:\n",
    "pca_act_training_records_reshaped = deepcopy(training_records[training_records.sensor_code=='act'])\n",
    "pca_act_training_records_reshaped[\"df\"] = [act_feature for act_feature in act_features]\n",
    "pca_act_training_records_reshaped[\"df\"] = pca_act_training_records_reshaped.df.apply(np.expand_dims,axis=0)\n",
    "\n",
    "pca_act_testing_records_reshaped = deepcopy(testing_records[testing_records.sensor_code=='act'])\n",
    "pca_act_testing_records_reshaped[\"df\"] = [act_test_feature for act_test_feature in act_test_features]\n",
    "pca_act_testing_records_reshaped[\"df\"] = pca_act_testing_records_reshaped.df.apply(np.expand_dims,axis=0)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3.2 Calculate PCA with 10 features for depth sensor data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Firstly resample the data\n",
    "(Similarity to the task 2.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.2\n",
    "### Your code begins here ###\n",
    "'''Work only with rows with dc'''\n",
    "dc_train_records = training_records[training_records['sensor'] == 'dc']\n",
    "dc_test_records = testing_records[testing_records['sensor'] == 'dc']\n",
    "\n",
    "'''Initialize PCA for depth senspr'''\n",
    "reduced_dimensions = 10\n",
    "pca_applier = PcaDcApplier(reduced_dimensions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Secondly perform the data standardization\n",
    "(Similarity to the task 2.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Standardize the dc data'''\n",
    "standardizer = Standardizer()\n",
    "standardizer.fit(dc_train_records)\n",
    "standardized_dc_train_records = standardizer.transform(dc_train_records)\n",
    "standardized_dc_test_records = standardizer.transform(dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Thirdly calculate the PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Fit and transform PCA'''\n",
    "pca_applier.fit(standardized_dc_train_records)\n",
    "\n",
    "pca_dc_train_records = pca_applier.transform(standardized_dc_train_records)\n",
    "pca_dc_test_records = pca_applier.transform(standardized_dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenate depth sensor data features by reshaping the 'df' dataframe\n",
    "We have 10 PCA components * 5 image samples = vector of 50 values per row"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Reshape the dataframes for both train and test datasets '''\n",
    "pca_dc_train_records_reshaped = deepcopy(pca_dc_train_records)\n",
    "pca_dc_train_records_reshaped[\"df\"] = pca_dc_train_records_reshaped[\"df\"].apply(reshape_dataframe)\n",
    "dc_features = np.concatenate(pca_dc_train_records_reshaped['df'].values,axis=0)\n",
    "\n",
    "pca_dc_test_records_reshaped = deepcopy(pca_dc_test_records)\n",
    "pca_dc_test_records_reshaped[\"df\"] = pca_dc_test_records_reshaped[\"df\"].apply(reshape_dataframe)\n",
    "dc_test_features = np.concatenate(pca_dc_test_records_reshaped['df'].values,axis=0)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Merge Accelerometer and Depth sensor PCA features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib, utilities\n",
    "importlib.reload(utilities.fun_one)\n",
    "from utilities.fun_one import visualize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Merge Accelerometer and Depth sensor PCA features\n",
    "\n",
    "Row index can't be used, as it's unique for each modality. Merge over 'subject_id', 'exercise_id', 'trial', 'window_idx' is done instead assuring, that Accelerometer and Depth sensor data are alligned properly. Merged data feature vector is of shape 1*86. 50 features from Depth sensor and 36 from Accelerometer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# 3.3\n",
    "### Your code begins here ###\n",
    "\n",
    "train_records_merged = pca_dc_train_records_reshaped.merge(pca_act_training_records_reshaped, on=['subject_id', 'exercise_id', 'trial', 'window_idx']).apply(merge_dataframes, axis=1)\n",
    "test_records_merged = pca_dc_test_records_reshaped.merge(pca_act_testing_records_reshaped, on=['subject_id', 'exercise_id', 'trial', 'window_idx']).apply(merge_dataframes, axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Save data for use in other notebooks:\n",
    "%store train_records_merged\n",
    "%store test_records_merged"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalize the data with Normalizer\n",
    "Min-max normalization of the merged data. Min and max are calculated for each feature in the feature vector to avoid bias among features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Normalize the features'''\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(train_records_merged)\n",
    "\n",
    "normalized_train_records = normalizer.transform(train_records_merged)\n",
    "normalized_test_records = normalizer.transform(test_records_merged)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initialization and training of the classifiers\n",
    "\n",
    "SVM and Gausian Naive Bayes classifiers are chosen for the classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Initialize and fit classifiers on training data'''\n",
    "gnb_classifier = GaussianNB()\n",
    "svm_classifier = SVC()\n",
    "\n",
    "gnb_classifier.fit(np.concatenate(normalized_train_records['df_normalized'].values,axis=0), normalized_train_records['exercise_id'].values)\n",
    "svm_classifier.fit(np.concatenate(normalized_train_records['df_normalized'].values,axis=0), normalized_train_records['exercise_id'].values)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Get classifier predictions for test and train datasets'''\n",
    "gnb_est_train_labels = gnb_classifier.predict(np.concatenate(normalized_train_records['df_normalized'].values,axis=0))\n",
    "gnb_est_test_labels = gnb_classifier.predict(np.concatenate(normalized_test_records['df_normalized'].values,axis=0))\n",
    "\n",
    "svm_est_train_labels = svm_classifier.predict(np.concatenate(normalized_train_records['df_normalized'].values,axis=0))\n",
    "svm_est_test_labels = svm_classifier.predict(np.concatenate(normalized_test_records['df_normalized'].values,axis=0))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Visualize classification results. First for Gaussian naiveBayes classifier, then for SVM classifier'''\n",
    "visualize(gnb_est_train_labels,\n",
    "          normalized_train_records['exercise_id'].values,\n",
    "          gnb_est_test_labels,\n",
    "          normalized_test_records['exercise_id'].values,\n",
    "          main_title=\"GNB merged\")\n",
    "\n",
    "visualize(svm_est_train_labels,\n",
    "          normalized_train_records['exercise_id'].values,\n",
    "          svm_est_test_labels,\n",
    "          normalized_test_records['exercise_id'].values,\n",
    "          main_title=\"SVM merged\")\n",
    "\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3.3 review\n",
    "\n",
    "As suggested in the review of Task 2.2, performances of classifiers on fused modalites data are much better, than the performance of the NN classifier on the data of separated modalities. Although 64% F1 score may still not be usable in the real world application improvement of almost 20% is big step forward. That's because of modality fusion and also more sophisticated classifier algorithms.\n",
    "\n",
    "SVM classifier classified almost 100% of training data correctly (unlike GNB with only \"92%\"), but the performance on the testing data was lower than GNB. This is probably the sign of over-fitted SVM to the training the data. Tuning the parrameters of the classifiers could improve the performance on the testing data even further.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
