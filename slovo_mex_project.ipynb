{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-708d46d3f9180abe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Multi-Modal Data Fusion - Project Work: Multi-Modal Physical Exercise Classification\n",
    "\n",
    "\n",
    "In this project, real multi-modal data is studied by utilizing different techniques presented during the course. In addition, there is an optional task to try some different approaches to identify persons from the same dataset. Open MEx dataset from UCI machine learning repository is used. Idea is to apply different techniques to recognize physical exercises from wearable sensors and depth camera, user-independently."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Author(s)\n",
    "Add your information here\n",
    "\n",
    "Name(s):\n",
    "Aleksander Madajczak,\n",
    "Jan Fabian\n",
    "\n",
    "Student number(s):\n",
    "2207367\n",
    "2207371\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description\n",
    "\n",
    "The goal of this project is to develop user-independent pre-processing and classification models to recognize 7 different physical exercises measured by accelerometer (attached to subject's thigh) and depth camera (above the subject facing downwards recording an aerial view). All the exercises were performed subject lying down on the mat. Original dataset have also another acceleration sensor and pressure-sensitive mat, but those two modalities are ommited in this project. There are totally 30 subjects in the original dataset, and in this work subset of 10 person is utilized. Detailed description of the dataset and original data can be access in [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#). We are providing the subset of dataset in Moodle.\n",
    "\n",
    "The project work is divided on following phases:\n",
    "\n",
    "1. Data preparation, exploration, and visualization\n",
    "2. Feature extraction and unimodal fusion for classification\n",
    "3. Feature extraction and feature-level fusion for multimodal classification\n",
    "4. Decision-level fusion for multimodal classification\n",
    "5. Bonus task: Multimodal biometric identification of persons\n",
    "\n",
    "where 1-4 are compulsory (max. 10 points each), and 5 is optional to get bonus points (max. 5+5 points). In each phase, you should visualize and analyse the results and document the work and findings properly by text blocks and figures between the code. <b> Nice looking </b> and <b> informative </b> notebook representing your results and analysis will be part of the grading in addition to actual implementation.\n",
    "\n",
    "The results are validated using confusion matrices and F1 scores. F1 macro score is given as\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{equation}\n",
    "F1_{macro} = \\frac{1}{N} \\sum_i^N F1_i,\n",
    "\\end{equation}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "where $F1_i = 2  \\frac{precision_i * recall_i}{precision_i + recall_i}$, and $N$ is the number of labels.\n",
    "<br>\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "After the project work, you should\n",
    "\n",
    "- be able to study real world multi-modal data\n",
    "- be able to apply different data fusion techniques to real-world problem\n",
    "- be able to evaluate the results\n",
    "- be able to analyse the outcome\n",
    "- be able to document your work properly\n",
    "\n",
    "## Relevant lectures\n",
    "\n",
    "Lectures 1-8\n",
    "\n",
    "## Relevant exercises\n",
    "\n",
    "Exercises 0-6\n",
    "\n",
    "## Relevant chapters in course book\n",
    "\n",
    "Chapter 1-14\n",
    "\n",
    "## Additional Material\n",
    "\n",
    "* Original dataset [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#)\n",
    "* Related scientific article [MEx: Multi-modal Exercises Dataset for Human Activity Recognition](https://arxiv.org/pdf/1908.08992.pdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data preparation, exploration, and visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='task1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 1.</b>\n",
    "\n",
    "Download data from the Moodle's Project section. Get yourself familiar with the folder structure and data. You can read the data files using the function given below. Each file consists one exercise type performed by single user. Data are divided on multiple folders. Note that, in each folder there is one long sequence of single exercise, except exercise 4 which is performed two times in different ways. Those two sequences belongs to same class. Do the following subtasks to pre-analyse data examples and to prepare the training and testing data for next tasks:\n",
    "<br>\n",
    "<br> \n",
    "<p> Read raw data from the files. Prepare and divide each data file to shorter sequences using windowing method. Similar to related article \"MEx: Multi-modal Exercises Dataset for Human Activity Recognition\", use 5 second window and 3 second overlapping between windows, producing several example sequences from one exercise file for classification purposes. Windowing is working so that starting from the beginning of each long exercise sequence, take 5 seconds of data points (from synchronized acceleration data and depth images) based on the time stamps. Next, move the window 2 seconds forward and take another 5 seconds of data. Then continue this until your are at the end of sequence. Each window will consists 500x3 matrix of acceleration data and 5x192 matrix of depth image data.</p>\n",
    "<br>  \n",
    "<p> <b>1.1</b> Plot few examples of prepared data for each modalities (accelometer and depth camera). Plot acceleration sensor as multi-dimensional time-series and depth camera data as 2D image. Plot 5 second acceleration sensor and depth image sequences of person 1 and 5 performing exercises 2, 5, and 6. Take the first windowed example from the long exercise sequence. </p>\n",
    "<br>\n",
    "<p> <b>1.2</b> Split the prepared dataset to training and testing datasets so that data of persons 1-7 are used for training and data of persons 8-10 are used for testing. In next tasks, training dataset could be further divided on (multiple) validation data folds to tune the models parameters, when needed.<br>\n",
    "    \n",
    "<p> Note: Training set should have 1486 windows and testing set should have 598 windows. In training set, acceleration data will have a window without a pair with depth camera data, that window should be dropped as it doesn't have a pair.<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Document your work, calculate the indicator statistics of training and testing datasets (number of examples, dimensions of each example) and visualize prepared examples.\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import relevant libraries here\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enter data folder location\n",
    "loc = \"./MEx\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def path_to_meta(p):\n",
    "    meta = dict()\n",
    "    meta[\"subject_id\"] = p.parent.stem\n",
    "    meta[\"exercise_id\"] = p.stem.split(\"_\")[-2]\n",
    "    meta[\"trial\"] = int(p.stem.split(\"_\")[-1])\n",
    "    meta[\"sensor_code\"] = p.stem.split(\"_\")[0]\n",
    "    meta[\"sensor\"] = {\"act\": \"acc\", \"dc\": \"dc\"}[meta[\"sensor_code\"]]\n",
    "    return meta\n",
    "\n",
    "# Find, read, and compose the measurements\n",
    "paths_record = Path(loc).glob(\"*/*/*.csv\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for path_record in paths_record:\n",
    "    df = pd.read_csv(path_record, delimiter=\",\", header=None)\n",
    "    meta = path_to_meta(path_record)\n",
    "    \n",
    "    if meta[\"sensor\"] == \"acc\":\n",
    "        col_names = [\"time\", \"acc_0\", \"acc_1\", \"acc_2\"]\n",
    "        df.columns = col_names\n",
    "    else:\n",
    "        num_cols = df.shape[1]\n",
    "        col_names = [\"time\", ] + [f\"dc_{i}\" for i in range(num_cols-1)]\n",
    "        df.columns = col_names\n",
    "\n",
    "    meta[\"df\"] = df\n",
    "    \n",
    "    records.append(meta)\n",
    "\n",
    "df_records = pd.DataFrame.from_records(records)\n",
    "\n",
    "print(f\"Total records found: {len(df_records)}\")\n",
    "print(\"Dataframe with all records:\")\n",
    "display(df_records.head())\n",
    "print(\"Dataframe with one measurement series:\")\n",
    "display(df_records[\"df\"].iloc[0].head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract 5-second long windows with 2-second shift (3-second overlap)\n",
    "\n",
    "records_windowed = []\n",
    "\n",
    "time_window = 5000.\n",
    "time_offset = 2000.\n",
    "    \n",
    "for row_idx, row_data in df_records.iterrows():\n",
    "    df_tmp = row_data[\"df\"]\n",
    "    time_start = np.min(df_tmp[\"time\"].to_numpy())\n",
    "    time_end = np.max(df_tmp[\"time\"].to_numpy())\n",
    "    \n",
    "    for window_idx, t0 in enumerate(np.arange(time_start, time_end, time_offset)):\n",
    "        t1 = t0 + time_window\n",
    "        # Handle boundary conditions - skip the measurements from the end shorter than window size\n",
    "        if t1 > time_end:\n",
    "            continue\n",
    "        \n",
    "        tmp_data = deepcopy(row_data)\n",
    "        tmp_data[\"window_idx\"] = window_idx\n",
    "        tmp_data[\"df\"] = df_tmp[(df_tmp[\"time\"] >= t0) &\n",
    "                                (df_tmp[\"time\"] < t1)].copy()\n",
    "        \n",
    "        records_windowed.append(tmp_data)\n",
    "        \n",
    "df_records_windowed = pd.DataFrame.from_records(records_windowed)\n",
    "\n",
    "print(f\"Total windows extracted: {len(df_records_windowed)}\")\n",
    "print(\"Dataframe with all windowed records:\")\n",
    "display(df_records_windowed.head())\n",
    "print(\"Dataframe with one windowed measurement series:\")\n",
    "display(df_records_windowed[\"df\"].iloc[0].head())\n",
    "%store df_records_windowed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.1. Visualize selected samples for both modalities\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "### Your code begins here ###\n",
    "def stringify_id(id,fill=2):\n",
    "    ''' Use to change int into the string with zeros prefix ex: 1 -> '01', 0 -> '00' 10 -> '10'. The fill arg determines the minimal number of characters in string (default 2)'''\n",
    "    return str(id).zfill(fill)\n",
    "\n",
    "def filter_dataframe(data,subject_id,exercise_id,sensor_id,window_id=0):\n",
    "    '''\n",
    "    Filter data, get only data with correct subject, exercise and sensor.\n",
    "    If using windowed data the window_id selects which window to return -\n",
    "    for non windowed data use 0.\n",
    "    '''\n",
    "    #First I create the filters:\n",
    "    right_subject = data.subject_id==stringify_id(subject_id) # Only those rows with matching subject id (subject id is saved as '0X' string so must be converted first)\n",
    "    right_exercise = data.exercise_id==stringify_id(exercise_id) # Only those rows with matching exercise id\n",
    "    right_sensor = data.sensor_code==sensor_id #get only depth camera entries or accelerometer readings\n",
    "    #Then I apply the filters:\n",
    "    data_frame = data[right_subject & right_exercise & right_sensor] # apply filters to the data frame\n",
    "    data_np = data_frame.df.iloc[window_id].to_numpy() # extract to numpy narray\n",
    "    return data_np\n",
    "\n",
    "def visualize_depth_series(data,subject_id,exercise_id,window_id=0):\n",
    "    '''\n",
    "    Visualize depth camera data,\n",
    "    the sensor readings are flattened and presented as a 2d timeseries\n",
    "    '''\n",
    "    depth_np = filter_dataframe(data,subject_id,exercise_id,'dc',window_id)\n",
    "    depth_time = depth_np[:,0] # get timestamps\n",
    "    depth_data = depth_np[:,1:] # get data and remove the first column with timestamps\n",
    "    f = plt.figure(figsize=(10,5))\n",
    "    plt.imshow(depth_data,cmap=plt.get_cmap('gray')) # plot the data with gray color pallet\n",
    "    plt.title(f\"Dc Full, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}\")\n",
    "    plt.yticks((depth_time[::5])/1000)\n",
    "    plt.ylabel(\"time [s]\")\n",
    "    plt.xlabel(\"channels\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_depth(data,subject_id,exercise_id,sample_id=None,window_id=0):\n",
    "    '''\n",
    "    Visualize depth camera data reading as 12x16 image,\n",
    "    which reading is plotted is controlled by sample_id\n",
    "    '''\n",
    "    depth_np = filter_dataframe(data,subject_id,exercise_id,'dc',window_id)\n",
    "    depth_time = depth_np[:,0] # get timestamps\n",
    "    depth_data = depth_np[:,1:] # get data and remove the first column with timestamps\n",
    "    anim = None\n",
    "    if sample_id is not None:\n",
    "        #Plot One Image:\n",
    "        sample_depth_img = depth_data[sample_id].reshape((12,16))\n",
    "        f = plt.figure(figsize=(10,5))\n",
    "        plt.imshow(sample_depth_img,cmap=plt.get_cmap('gray')) # plot the data with gray color pallet\n",
    "        plt.title(f\"Dc, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}, s{sample_id}\")\n",
    "        plt.xticks(list(range(0,16,2))+ [15])\n",
    "        plt.yticks(list(range(0,12,2))+ [11])\n",
    "        #plt.margins(x=0, y=0)\n",
    "        plt.show()\n",
    "    else:\n",
    "        #Plot Image series:\n",
    "        f, ax = plt.subplots()\n",
    "        images = []\n",
    "        for i,img in enumerate(depth_data):\n",
    "            image = img.reshape(12,16)\n",
    "            im = ax.imshow(image, cmap=plt.get_cmap('gray'), animated=True) # plot the data with gray color pallet\n",
    "            title_text = f\"Dc, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}, s{str(i)}, t{(depth_time[i]/1000):.0f}[s]\"\n",
    "            title = ax.text(0.5,1.05, title_text,\n",
    "                    size=plt.rcParams[\"axes.titlesize\"],\n",
    "                    ha=\"center\", transform=ax.transAxes)\n",
    "            if i == 0:\n",
    "               ax.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "               #continue\n",
    "            images.append([im,title])\n",
    "\n",
    "        plt.xticks(list(range(0,16,2))+ [15])\n",
    "        plt.yticks(list(range(0,12,2))+ [11])\n",
    "        anim = animation.ArtistAnimation(f, images, interval=1000, blit=True, repeat_delay=1000)\n",
    "        plt.show()\n",
    "        return anim\n",
    "\n",
    "def visualize_acceleration(data,subject_id,exercise_id,window_id=0):\n",
    "    '''\n",
    "    Visualize acceleration readings as as 3 timeseries.\n",
    "    '''\n",
    "    accele_np = filter_dataframe(data,subject_id,exercise_id,'act',window_id)\n",
    "    accele_time = accele_np[:,0] # get timestamps 100Hz\n",
    "    accele_d1 = accele_np[:,1] # get first timeseries x\n",
    "    accele_d2 = accele_np[:,2] # get second timeseries y\n",
    "    accele_d3 = accele_np[:,3] # get third timeseries z\n",
    "    f = plt.figure(figsize=(10,5))\n",
    "    plt.plot(accele_time,accele_d1,c='blue',label='x') # plot first timeseries\n",
    "    plt.plot(accele_time,accele_d2,c='orange',label='y') # plot second timeseries\n",
    "    plt.plot(accele_time,accele_d3,c='red',label='z') # plot third timeseries\n",
    "    plt.title(f\"Act, Sub{str(subject_id).zfill(2)}, Exe{str(exercise_id).zfill(2)}\")\n",
    "    plt.xticks(accele_time[::1000],(accele_time[::1000])*0.01)\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    plt.ylabel(\"acceleration [g]\") # g - of earths acceleration\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize later classification results:\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "def visualize(train_preds,train_labels,test_preds,test_labels,main_title,scale=1):\n",
    "    train_matrix = metrics.confusion_matrix(np.array(train_labels), train_preds)\n",
    "    train_f1 = metrics.f1_score(np.array(train_labels),train_preds,average='macro')\n",
    "    test_matrix = metrics.confusion_matrix(np.array(test_labels), test_preds)\n",
    "    test_f1 = metrics.f1_score(np.array(test_labels),test_preds,average='macro')\n",
    "    print(main_title)\n",
    "    print(\"Training data F1 score = \", train_f1)\n",
    "    print(\"Testing data F1 score = \", test_f1)\n",
    "    #Plot the matrixes:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15*scale, 5*scale))\n",
    "    fig.suptitle(main_title, fontsize=15*scale)\n",
    "    ax[0].set_title('',fontsize= 10 * (1/(1 + np.exp(-10*scale + 4.25))))\n",
    "    ax[0].title.set_text(\"Training data confusion matrix:\")\n",
    "    ev_mat_disp = metrics.ConfusionMatrixDisplay(train_matrix)\n",
    "    ev_mat_disp.plot(ax=ax[0])\n",
    "    ax[1].set_title('',fontsize= 10 * (1/(1 + np.exp(-10*scale + 4.25))))\n",
    "    ax[1].title.set_text(\"Testing data confusion matrix:\")\n",
    "    ts_mat_disp = metrics.ConfusionMatrixDisplay(test_matrix)\n",
    "    ts_mat_disp.plot(ax=ax[1])\n",
    "    plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualisation - Acceleration:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(\"Acceleration, Person 1, Exercise 2:\")\n",
    "visualize_acceleration(df_records_windowed,1,2)\n",
    "print(\"Acceleration, Person 1, Exercise 5:\")\n",
    "visualize_acceleration(df_records_windowed,1,5)\n",
    "print(\"Acceleration, Person 1, Exercise 6:\")\n",
    "visualize_acceleration(df_records_windowed,1,6)\n",
    "print(\"Acceleration, Person 5, Exercise 2:\")\n",
    "visualize_acceleration(df_records_windowed,5,2)\n",
    "print(\"Acceleration, Person 5, Exercise 5:\")\n",
    "visualize_acceleration(df_records_windowed,5,5)\n",
    "print(\"Acceleration, Person 5, Exercise 6:\")\n",
    "visualize_acceleration(df_records_windowed,5,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualisation - Depth:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(\"Visualisation Depth Series, First Window:\")\n",
    "print(\"Person 1, Exercise 2:\")\n",
    "visualize_depth_series(df_records_windowed,1,2)\n",
    "print(\"Person 1, Exercise 5:\")\n",
    "visualize_depth_series(df_records_windowed,1,5)\n",
    "print(\"Person 1, Exercise 6:\")\n",
    "visualize_depth_series(df_records_windowed,1,6)\n",
    "print(\"Person 5, Exercise 2:\")\n",
    "visualize_depth_series(df_records_windowed,5,2)\n",
    "print(\"Person 5, Exercise 5:\")\n",
    "visualize_depth_series(df_records_windowed,5,5)\n",
    "print(\"Person 5, Exercise 6:\")\n",
    "visualize_depth_series(df_records_windowed,5,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 2:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,2,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 5:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,5,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 1, Exercise 6:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,1,6,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 2:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,2,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 5:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,5,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Person 5, Exercise 6:\")\n",
    "%matplotlib notebook\n",
    "visualize_depth(df_records_windowed,5,6,None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# 1.2. Split samples based on subject ID into training and testing datasets for futher experiments\n",
    "print(\"Splitting data into train ( persons 1-7 ) and test set (persons 8-10)\")\n",
    "### Your code begins here ###\n",
    "training_records = df_records_windowed[(df_records_windowed.subject_id==stringify_id(1)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(2)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(3)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(4)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(5)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(6)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(7)) ]\n",
    "testing_records  = df_records_windowed[(df_records_windowed.subject_id==stringify_id(8)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(9)) |\n",
    "                                       (df_records_windowed.subject_id==stringify_id(10))]\n",
    "#Drop one row from training set which does not have a pair of sensor readings:\n",
    "training_records = training_records.drop(training_records.index[(training_records.subject_id==stringify_id(2) ) &\n",
    "                                             (training_records.exercise_id==stringify_id(6) ) &\n",
    "                                             (training_records.sensor_code=='act' )  &\n",
    "                                             (training_records.window_idx==29 )])\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testing_records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Feature extraction and fusion for unimodal classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='task2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 2.</b>\n",
    "\n",
    "Use the training dataset prepared in task 1. to build models based on the combination of principal component analysis (PCA), linear discriminant analysis (LDA), and nearest neighbour (NN) classifier for each modality separately and evaluate the model on test dataset. Do the subtasks given as\n",
    "<br>\n",
    "<br>\n",
    "<p> <b>2.1</b> Calculate PCA and LDA transformations to reduce the dimensionality of accelerometer data (e.g., using scikit-learn implementations). Before transformations downsample data from 100 Hz to 25 Hz (using scipy.signal.resample) to get 125x3 matrix of data for each 5 sec window. You should also standardize the values to zero mean and unit variance before the transformations. Using training dataset, fit PCA with 5-dimensional subspace (i.e., choosing the 5 largest principal components) and fit LDA with 5-dimensional subspace. Transform both train and test examples to this low-dimensional feature representation. Concatenate each sequence to single vector size of 3x(5+5). Perform the fusion of PCA and LDA similar manner as presented in Lecture 3 (pages 24-25) using NN method. Evaluate the performance on testset. Show confusion matrix and F1 scores of the results. </p>\n",
    "<br>\n",
    "<p> <b>2.2</b> Use PCA and LDA transformations to reduce the dimensionality of depth images. You should also standardize the values to zero mean and unit variance before the transformations. Fit PCA and LDA for all training images (12x16, 192-dimensional in vectorized form) by choosing 5-dimensional subspace for both PCA and LDA. Transform both train and test examples to this low-dimensional feature representation. Concatenate each sequence to single vector size of 5x1x(5+5). Similar to task 2.1, do the PCA and LDA fusion using NN and evaluate the performance on testset. Show confusion matrix and F1 scores of the results. </p>\n",
    "<br> \n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 2.1-2.2.\n",
    "    \n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Task 2 imports'''\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, ConfusionMatrixDisplay\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def acccelerometer_resample(data,n_samples=125):\n",
    "    return data[data.sensor_code=='act'].df.apply(\n",
    "            lambda x: pd.DataFrame().assign(\n",
    "                act_0=signal.resample(x.acc_0,n_samples),\n",
    "                act_1=signal.resample(x.acc_1,n_samples),\n",
    "                act_2=signal.resample(x.acc_2,n_samples)))\n",
    "\n",
    "class Standardizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data,df_name='df') -> None:\n",
    "        '''Concatenate all dataframes to one to calculate mean and std'''\n",
    "        samples_concatenated = pd.concat(data[df_name].values, ignore_index=True)\n",
    "        if 'time' in samples_concatenated:\n",
    "            samples_concatenated = samples_concatenated.drop('time', axis=1)\n",
    "        self.mean = np.mean(samples_concatenated, axis=0)\n",
    "        self.std = np.std(samples_concatenated, axis=0)\n",
    "\n",
    "    def transform(self, data,df_name='df'):\n",
    "        standardized_data = deepcopy(data)\n",
    "\n",
    "        for index, row in standardized_data.iterrows():\n",
    "            df = row[df_name]\n",
    "            if 'time' in df:\n",
    "                df = df.drop('time', axis=1)\n",
    "            df_standardized = (df - self.mean) / self.std\n",
    "            standardized_data.at[index,df_name] = df_standardized\n",
    "        return standardized_data\n",
    "\n",
    "    def display_mean_and_std(self) -> None:\n",
    "        display(self.mean)\n",
    "        display(self.std)\n",
    "\n",
    "class PcaActApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.pca = [PCA(n_components), PCA(n_components), PCA(n_components)]\n",
    "\n",
    "    def fit(self, data) -> None:\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        self.pca[0].fit(x[0])\n",
    "        self.pca[1].fit(x[1])\n",
    "        self.pca[2].fit(x[2])\n",
    "\n",
    "    def transform(self, data):\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        return self.pca[0].transform(x[0]), self.pca[1].transform(x[1]), self.pca[2].transform(x[2])\n",
    "\n",
    "    def get_pca(self):\n",
    "        return self.pca\n",
    "\n",
    "class LdaActApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.lda = LDA(n_components = n_components), LDA(n_components = n_components), LDA(n_components = n_components)\n",
    "\n",
    "    def fit(self, data, labels) -> None:\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        self.lda[0].fit(x[0],labels)\n",
    "        self.lda[1].fit(x[1],labels)\n",
    "        self.lda[2].fit(x[2],labels)\n",
    "\n",
    "    def transform(self, data):\n",
    "        x = data.apply(lambda x: x.act_0.T), data.apply(lambda x: x.act_1.T), data.apply(lambda x: x.act_2.T)\n",
    "        return self.lda[0].transform(x[0]), self.lda[1].transform(x[1]), self.lda[2].transform(x[2])\n",
    "\n",
    "    def get_lda(self):\n",
    "        return self.lda\n",
    "\n",
    "def act_fusion(act_pca_train, act_lda_train, act_pca_test, act_lda_test, train_labels):\n",
    "    #Combine data to array:\n",
    "    cobined  = np.concatenate((act_pca_train[0],act_pca_train[1],act_pca_train[2],act_lda_train[0],act_lda_train[1],act_lda_train[2]),axis=1)\n",
    "    test_cobined  = np.concatenate((act_pca_test[0],act_pca_test[1],act_pca_test[2],act_lda_test[0],act_lda_test[1],act_lda_test[2]),axis=1)\n",
    "    # Fusion\n",
    "    labels = np.zeros(test_cobined.shape[0])\n",
    "    for i,sample in enumerate(test_cobined): # chose one data point to classify # (N, K,  xyz )\n",
    "        d = np.zeros(cobined.shape[0])\n",
    "        D = np.zeros(cobined.shape[0])\n",
    "        for n in range(0,cobined.shape[0]): # Iterate over all samples\n",
    "            d[n] = np.sum( [ (sample[k] - cobined[n][k])**2 for k in range(0,15) ],axis=0)\n",
    "            D[n] = np.sum( [ (sample[k] - cobined[n][k])**2 for k in range(15,30)],axis=0)\n",
    "        d = (d - np.min(d)) / ( np.max(d) - np.min(d))\n",
    "        D = (D - np.min(D)) / ( np.max(D) - np.min(D))\n",
    "        F = 0.5 * (d + D)\n",
    "        n_star = np.argmin(F)\n",
    "        label = train_labels.to_numpy()[n_star] # array with exercise_id\n",
    "        labels[i] = label\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# 2.1\n",
    "### Your code begins here ###\n",
    "# Resample data:\n",
    "resample_samples = 125\n",
    "act_train = pd.DataFrame()\n",
    "act_train['df'] = acccelerometer_resample(training_records,resample_samples)\n",
    "train_labels = training_records[training_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "act_test = pd.DataFrame()\n",
    "act_test['df'] = acccelerometer_resample(testing_records,resample_samples)\n",
    "test_labels = testing_records[testing_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "\n",
    "#Standardize the Data:\n",
    "act_s = Standardizer()\n",
    "act_s.fit(act_train)\n",
    "act_train['df'] = act_s.transform(act_train)\n",
    "act_test['df'] = act_s.transform(act_test)\n",
    "\n",
    "#PCA\n",
    "n_components = 5\n",
    "pca = PcaActApplier(n_components)\n",
    "pca.fit(act_train['df'])\n",
    "act_pca_train = pca.transform(act_train['df'])\n",
    "act_pca_test = pca.transform(act_test['df'])\n",
    "\n",
    "#LDA:\n",
    "n_components = 5\n",
    "lda = LdaActApplier(n_components)\n",
    "lda.fit(act_train['df'],train_labels)\n",
    "act_lda_train = lda.transform(act_train['df'])\n",
    "act_lda_test = lda.transform(act_test['df'])\n",
    "\n",
    "# Fusion:\n",
    "train_pred_labels = act_fusion(act_pca_train,act_lda_train,act_pca_train,act_lda_train,train_labels)\n",
    "test_pred_labels = act_fusion(act_pca_train,act_lda_train,act_pca_test,act_lda_test,train_labels)\n",
    "\n",
    "# Visualisation:\n",
    "visualize(train_pred_labels,train_labels,test_pred_labels,test_labels,main_title=\"Accelerometer sensor used to classify the exercises\")\n",
    "\n",
    "### Your code ends here ###\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Task 2.2 helpers'''\n",
    "#helper class\n",
    "class PcaDcApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.pca = PCA(n_components)\n",
    "\n",
    "    def fit(self, data, df_name='df') -> None:\n",
    "        '''Concatenate all dataframes to one'''\n",
    "        samples_concatenated = pd.concat(data[df_name].values, ignore_index=True)\n",
    "        self.pca.fit(samples_concatenated)\n",
    "\n",
    "    def transform(self, data,df_name='df'):\n",
    "        pca_transformed_data = deepcopy(data)\n",
    "        pca_transformed_data[df_name] = pca_transformed_data[df_name].apply(self.pca.transform)\n",
    "        return pca_transformed_data\n",
    "\n",
    "class LdaDcApplier:\n",
    "    def __init__(self, n_components) -> None:\n",
    "        self.lda = LDA(n_components = n_components)\n",
    "\n",
    "    def fit(self, data, df_name='df') -> None:\n",
    "        samples_concatenated = pd.concat(data[df_name].values, ignore_index=True)\n",
    "        labels = []\n",
    "        time_window_length = data[df_name].values[0].shape[0]\n",
    "\n",
    "        for index, value in data['exercise_id'].values:\n",
    "            for i in range(0, time_window_length):\n",
    "                labels = np.append(labels,value)\n",
    "\n",
    "        self.lda.fit(samples_concatenated, labels)\n",
    "\n",
    "    def transform(self, data, df_name='df'):\n",
    "        lda_transformed_data = deepcopy(data)\n",
    "        lda_transformed_data[df_name] = lda_transformed_data[df_name].apply(self.lda.transform)\n",
    "        return lda_transformed_data\n",
    "\n",
    "#helper methods\n",
    "'''def concat_pca_lda(pca_data, lda_data):\n",
    "    concatenated_data = deepcopy(pca_data)\n",
    "    for index in concatenated_data.index:\n",
    "        df_concatenated = pd.concat([pca_data['df'].loc[index], lda_data['df'].loc[index]], axis=1, ignore_index=True)\n",
    "        concatenated_data.at[index,'df'] = df_concatenated\n",
    "    return concatenated_data'''\n",
    "\n",
    "'''def classifyNN(train_data, test_data):\n",
    "    pca_range = range(0,5)  # pca features are in columns 0-5\n",
    "    lda_range = range(5,10) # lda features are in columns 5-10\n",
    "\n",
    "    estimated_test_data_labels = pd.DataFrame([], columns=['test_item_index', 'real_label', 'estimated_label'])\n",
    "\n",
    "    for test_index, test_item in test_data.iterrows():\n",
    "        pca_test = test_item['df'].iloc[:,pca_range].values\n",
    "        lda_test = test_item['df'].iloc[:,lda_range].values\n",
    "\n",
    "\n",
    "        distances = pd.DataFrame([], columns=['train_item_index', 'dn_pca', 'Dn_lda'])\n",
    "        for train_index, train_item in train_data.iterrows():\n",
    "            pca_train = train_item['df'].iloc[:,pca_range].values\n",
    "            lda_train = train_item['df'].iloc[:,lda_range].values\n",
    "            dn_pca = np.sum(np.sum((pca_test - pca_train)**2, axis=1)) #TODO idk if this is good\n",
    "            Dn_lda = np.sum(np.sum((lda_test - lda_train)**2, axis=1))\n",
    "\n",
    "            new_distance = pd.DataFrame({'train_item_index': train_index,'dn_pca': [dn_pca], 'Dn_lda': [Dn_lda]})\n",
    "            distances = pd.concat([distances, new_distance], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "        min_dn_pca = np.min(distances['dn_pca'])\n",
    "        max_dn_pca = np.max(distances['dn_pca'])\n",
    "        min_Dn_lda = np.min(distances['Dn_lda'])\n",
    "        max_Dn_lda = np.max(distances['Dn_lda'])\n",
    "\n",
    "        distances['scaled_dn_pca'] = (distances['dn_pca'] - min_dn_pca)/(max_dn_pca - min_dn_pca)\n",
    "        distances['scaled_Dn_lda'] = (distances['Dn_lda'] - min_Dn_lda)/(max_Dn_lda - min_Dn_lda)\n",
    "\n",
    "        distances['fused_distances'] = 0.5*(distances['scaled_dn_pca']+distances['scaled_Dn_lda'])\n",
    "\n",
    "        real_label = test_item['exercise_id']\n",
    "        estimated_label = train_data['exercise_id'].loc[distances['train_item_index'].iloc[np.argmin(distances['fused_distances'])]]\n",
    "\n",
    "        new_estimation = pd.DataFrame([{'test_item_index': test_index,'real_label': real_label, 'estimated_label': estimated_label}])\n",
    "        estimated_test_data_labels = pd.concat([estimated_test_data_labels, new_estimation], axis=0, ignore_index=True)\n",
    "    return estimated_test_data_labels'''\n",
    "\n",
    "\n",
    "def concat_pca_lda(pca_data, lda_data,df_name='df'):\n",
    "    concatenated_data = deepcopy(pca_data)\n",
    "    for index in concatenated_data.index:\n",
    "        pca_reshaped = pca_data[df_name].loc[index].reshape(1,-1)\n",
    "        lda_reshaped = lda_data[df_name].loc[index].reshape(1,-1)\n",
    "        df_concatenated = np.concatenate([pca_reshaped, lda_reshaped], axis=1)\n",
    "        concatenated_data.at[index,df_name] = df_concatenated.flatten()\n",
    "    return concatenated_data\n",
    "\n",
    "def classifyNN(train_data, test_data):\n",
    "    pca_range = range(0,25)  # pca features are in columns 0-5\n",
    "    lda_range = range(25,50) # lda features are in columns 5-10\n",
    "\n",
    "    estimated_test_data_labels = pd.DataFrame([], columns=['real_label', 'estimated_label'])\n",
    "\n",
    "    #iterate over all samples to classify\n",
    "    for test_index, test_item in test_data.iterrows():\n",
    "        pca_test = test_item['df'][pca_range]\n",
    "        lda_test = test_item['df'][lda_range]\n",
    "\n",
    "        distances = pd.DataFrame([], columns=['dn_pca', 'Dn_lda'])\n",
    "        for train_index, train_item in train_data.iterrows():\n",
    "            pca_train = train_item['df'][pca_range]\n",
    "            lda_train = train_item['df'][lda_range]\n",
    "            dn_pca = np.sum((pca_test - pca_train)**2)\n",
    "            Dn_lda = np.sum((lda_test - lda_train)**2)\n",
    "\n",
    "            distances.at[train_index, 'dn_pca'] = dn_pca\n",
    "            distances.at[train_index, 'Dn_lda'] = Dn_lda\n",
    "\n",
    "        # calculate minimal and maximal distances\n",
    "        min_dn_pca = np.min(distances['dn_pca'])\n",
    "        max_dn_pca = np.max(distances['dn_pca'])\n",
    "        min_Dn_lda = np.min(distances['Dn_lda'])\n",
    "        max_Dn_lda = np.max(distances['Dn_lda'])\n",
    "\n",
    "        # scale distances\n",
    "        distances['dn_pca'] = (distances['dn_pca'] - min_dn_pca)/(max_dn_pca - min_dn_pca)\n",
    "        distances['Dn_lda'] = (distances['Dn_lda'] - min_Dn_lda)/(max_Dn_lda - min_Dn_lda)\n",
    "\n",
    "        # fuse distances\n",
    "        distances['fused_distances'] = 0.5*(distances['dn_pca']+distances['Dn_lda'])\n",
    "\n",
    "        # save labels\n",
    "        estimated_test_data_labels.at[test_index, 'real_label'] = test_item['exercise_id']\n",
    "        estimated_test_data_labels.at[test_index, 'estimated_label'] = train_data['exercise_id'].loc[distances.index[np.argmin(distances['fused_distances'])]]\n",
    "    return estimated_test_data_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.2\n",
    "### Your code begins here ###\n",
    "start_time = time.time()\n",
    "\n",
    "'''Work only with rows with dc'''\n",
    "dc_train_records = training_records[training_records['sensor'] == 'dc']\n",
    "dc_test_records = testing_records[testing_records['sensor'] == 'dc']\n",
    "\n",
    "'''Initialize PCA and LDA for acc sensor'''\n",
    "reduced_dimensions = 5\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Standardize the dc data'''\n",
    "standardizer = Standardizer()\n",
    "standardizer.fit(dc_train_records)\n",
    "standardized_dc_train_records = standardizer.transform(dc_train_records)\n",
    "standardized_dc_test_records = standardizer.transform(dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Fit PCA for depth senspr on standardized train dataset and transform test an train dataset'''\n",
    "pca_applier = PcaDcApplier(reduced_dimensions)\n",
    "pca_applier.fit(standardized_dc_train_records)\n",
    "\n",
    "pca_dc_train_records = pca_applier.transform(standardized_dc_train_records)\n",
    "pca_dc_test_records = pca_applier.transform(standardized_dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Fit PCA for depth senspr on standardized train dataset and transform test an train dataset'''\n",
    "lda_applier = LdaDcApplier(reduced_dimensions)\n",
    "lda_applier.fit(standardized_dc_train_records)\n",
    "\n",
    "lda_dc_train_records = lda_applier.transform(standardized_dc_train_records)\n",
    "lda_dc_test_records = lda_applier.transform(standardized_dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "'''Concatenate PCA and LDA features of depth sensor data'''\n",
    "concat_dc_train_records = concat_pca_lda(pca_dc_train_records, lda_dc_train_records)\n",
    "concat_dc_test_records = concat_pca_lda(pca_dc_test_records, lda_dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Get estimated labels for train dataset calculated by Nearest Neighbour algorithm on depth sensor data'''\n",
    "est_train_labels = classifyNN(train_data = concat_dc_train_records, test_data=concat_dc_train_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Get estimated labels for test dataset calculated by Nearest Neighbour algorithm on depth sensor data'''\n",
    "est_test_labels = classifyNN(train_data = concat_dc_train_records, test_data=concat_dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Visualize the results of NN classification for depth sensor data'''\n",
    "visualize(est_train_labels['estimated_label'],est_train_labels['real_label'],est_test_labels['estimated_label'],est_test_labels['real_label'],main_title=\"Depth sensor used to classify the exercises\")\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Feature extraction and feature-level fusion for multimodal classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='task3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 3.</b>\n",
    "\n",
    "Prepare new feature sets for each modality and combine them to single feature representation. Compare two classifiers from scikit-learn. Train classifiers using joint feature presentation. Evaluate and compare the result using testing dataset. Do the subtasks given as\n",
    "<br>   \n",
    "<br> \n",
    "<p> <b>3.1</b> Similar to task 2.1, calculate PCA for accelerometer, but choose now the 10 largest principal components as 10-dim feature vector for each window. In addition, for each window calculate mean and standard deviation of each three acc channels as statistical features, resulting 6-dimensional vector. Combine these to 36-dimensional final feature vector.</p>\n",
    "<br>  \n",
    "<p> <b>3.2</b> Similar to task 2.2, calculate the PCA for depth images using same setup, but now choose the 10 largest principal components as feature vector. Concatenate the image sequence forming 50-dimensional feature vector from each windowed example.</p>\n",
    "<br> \n",
    "<p> <b>3.3</b> Form a joint feature presentation of features extracted in 3.1 and 3.2, resulting 86-dimensional feature vector for each example. Normalize data between 0-1 using the training dataset. Use support vector machine (SVM) with RBF-kernel and Gaussian naiveBayes classifier (use default parameter values for both classifiers). Train the classifiers and evaluate and compare classifiers on testset using confusion matrices and F1 scores.</p>\n",
    "<br> \n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 3.1-3.3.\n",
    "    \n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# 3.1\n",
    "### Your code begins here ###\n",
    "# Resample data:\n",
    "resample_samples = 125\n",
    "act_train = pd.DataFrame()\n",
    "act_train['df'] = acccelerometer_resample(training_records,resample_samples)\n",
    "train_labels = training_records[training_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "act_test = pd.DataFrame()\n",
    "act_test['df'] = acccelerometer_resample(testing_records,resample_samples)\n",
    "test_labels = testing_records[testing_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x))\n",
    "\n",
    "#Standardize the Data:\n",
    "act_s = Standardizer()\n",
    "act_s.fit(act_train)\n",
    "act_train['df'] = act_s.transform(act_train)\n",
    "act_test['df'] = act_s.transform(act_test)\n",
    "\n",
    "\n",
    "n_components = 10\n",
    "pca = PcaActApplier(n_components)\n",
    "pca.fit(act_train['df'])\n",
    "act_pca_train = pca.transform(act_train['df'])\n",
    "act_pca_test = pca.transform(act_test['df'])\n",
    "\n",
    "act_features = np.concatenate((act_pca_train[0],act_pca_train[1],act_pca_train[2],\n",
    "                               np.mean(act_pca_train[0],axis=1).reshape(-1,1), np.std(act_pca_train[0],axis=1).reshape(-1,1),\n",
    "                               np.mean(act_pca_train[1],axis=1).reshape(-1,1), np.std(act_pca_train[1],axis=1).reshape(-1,1),\n",
    "                               np.mean(act_pca_train[2],axis=1).reshape(-1,1), np.std(act_pca_train[2],axis=1).reshape(-1,1),\n",
    "                              ),axis=1)\n",
    "\n",
    "act_test_features = np.concatenate((act_pca_test[0],act_pca_test[1],act_pca_test[2],\n",
    "                                    np.mean(act_pca_test[0],axis=1).reshape(-1,1), np.std(act_pca_test[0],axis=1).reshape(-1,1),\n",
    "                                    np.mean(act_pca_test[1],axis=1).reshape(-1,1), np.std(act_pca_test[1],axis=1).reshape(-1,1),\n",
    "                                    np.mean(act_pca_test[2],axis=1).reshape(-1,1), np.std(act_pca_test[2],axis=1).reshape(-1,1),\n",
    "                                   ),axis=1)\n",
    "\n",
    "pca_act_training_records_reshaped = deepcopy(training_records[training_records.sensor_code=='act'])\n",
    "pca_act_training_records_reshaped[\"df\"] = [act_feature for act_feature in act_features]\n",
    "pca_act_training_records_reshaped[\"df\"] = pca_act_training_records_reshaped.df.apply(np.expand_dims,axis=0)\n",
    "\n",
    "pca_act_testing_records_reshaped = deepcopy(testing_records[testing_records.sensor_code=='act'])\n",
    "pca_act_testing_records_reshaped[\"df\"] = [act_test_feature for act_test_feature in act_test_features]\n",
    "pca_act_testing_records_reshaped[\"df\"] = pca_act_testing_records_reshaped.df.apply(np.expand_dims,axis=0)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.2\n",
    "### Your code begins here ###\n",
    "'''Work only with rows with dc'''\n",
    "dc_train_records = training_records[training_records['sensor'] == 'dc']\n",
    "dc_test_records = testing_records[testing_records['sensor'] == 'dc']\n",
    "\n",
    "'''Initialize PCA for depth senspr'''\n",
    "reduced_dimensions = 10\n",
    "pca_applier = PcaDcApplier(reduced_dimensions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Standardize the dc data'''\n",
    "standardizer = Standardizer()\n",
    "standardizer.fit(dc_train_records)\n",
    "standardized_dc_train_records = standardizer.transform(dc_train_records)\n",
    "standardized_dc_test_records = standardizer.transform(dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Fit and transform PCA'''\n",
    "pca_applier.fit(standardized_dc_train_records)\n",
    "\n",
    "pca_dc_train_records = pca_applier.transform(standardized_dc_train_records)\n",
    "pca_dc_test_records = pca_applier.transform(standardized_dc_test_records)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Helper method for reshaping dataframe with PCA feature values of recording window from 5x10 to 1x50\n",
    "def reshape_dataframe(df):\n",
    "    return df.reshape(1, -1)\n",
    "\n",
    "'''Reshape the dataframes for both train and test datasets'''\n",
    "pca_dc_train_records_reshaped = deepcopy(pca_dc_train_records)\n",
    "pca_dc_train_records_reshaped[\"df\"] = pca_dc_train_records_reshaped[\"df\"].apply(reshape_dataframe)\n",
    "dc_features = np.concatenate(pca_dc_train_records_reshaped['df'].values,axis=0)\n",
    "\n",
    "pca_dc_test_records_reshaped = deepcopy(pca_dc_test_records)\n",
    "pca_dc_test_records_reshaped[\"df\"] = pca_dc_test_records_reshaped[\"df\"].apply(reshape_dataframe)\n",
    "dc_test_features = np.concatenate(pca_dc_test_records_reshaped['df'].values,axis=0)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Task 3.3 helper methods'''\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.min= None\n",
    "        self.max = None\n",
    "\n",
    "    def fit(self, data) -> None:\n",
    "        '''Concatenate all dataframes to one to calculate min and max'''\n",
    "        samples_concatenated = np.concatenate(train_records_merged['df_merged'].values,axis=0)\n",
    "        self.min = np.min(samples_concatenated, axis=0).reshape(1,-1)\n",
    "        self.max = np.max(samples_concatenated, axis=0).reshape(1,-1)\n",
    "\n",
    "    def normalize_row(self,row):\n",
    "        row['df_normalized'] = (row['df_merged'] - self.min) / (self.max - self.min)\n",
    "        return row\n",
    "\n",
    "    def transform(self, data):\n",
    "        normalized_data = deepcopy(data)\n",
    "        normalized_data = normalized_data.apply(self.normalize_row, axis=1)\n",
    "        return normalized_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# 3.3\n",
    "### Your code begins here ###\n",
    "'''Merge PCA of from accelerometer and depth sensor to one feature vector with 86 fetures'''\n",
    "def merge_dataframes(row):\n",
    "    row['df_merged'] = np.concatenate([row['df_x'], row['df_y']], axis=1)\n",
    "    return row\n",
    "\n",
    "train_records_merged = pca_dc_train_records_reshaped.merge(pca_act_training_records_reshaped, on=['subject_id', 'exercise_id', 'trial', 'window_idx']).apply(merge_dataframes, axis=1)\n",
    "test_records_merged = pca_dc_test_records_reshaped.merge(pca_act_testing_records_reshaped, on=['subject_id', 'exercise_id', 'trial', 'window_idx']).apply(merge_dataframes, axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Normalize the features'''\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(train_records_merged)\n",
    "\n",
    "normalized_train_records = normalizer.transform(train_records_merged)\n",
    "normalized_test_records = normalizer.transform(test_records_merged)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Initialize and fit classifiers on training data'''\n",
    "gnb_classifier = GaussianNB()\n",
    "svm_classifier = SVC()\n",
    "\n",
    "gnb_classifier.fit(np.concatenate(normalized_train_records['df_normalized'].values,axis=0), normalized_train_records['exercise_id'].values)\n",
    "svm_classifier.fit(np.concatenate(normalized_train_records['df_normalized'].values,axis=0), normalized_train_records['exercise_id'].values)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''Get classifier predictions for test and train datasets'''\n",
    "gnb_est_train_labels = gnb_classifier.predict(np.concatenate(normalized_train_records['df_normalized'].values,axis=0))\n",
    "gnb_est_test_labels = gnb_classifier.predict(np.concatenate(normalized_test_records['df_normalized'].values,axis=0))\n",
    "\n",
    "svm_est_train_labels = svm_classifier.predict(np.concatenate(normalized_train_records['df_normalized'].values,axis=0))\n",
    "svm_est_test_labels = svm_classifier.predict(np.concatenate(normalized_test_records['df_normalized'].values,axis=0))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''Visualize classification results. First for Gaussian naiveBayes classifier, then for SVM classifier'''\n",
    "visualize(gnb_est_train_labels,\n",
    "          normalized_train_records['exercise_id'].values,\n",
    "          gnb_est_test_labels,\n",
    "          normalized_test_records['exercise_id'].values,\n",
    "          main_title=\"GNB merged\")\n",
    "\n",
    "visualize(svm_est_train_labels,\n",
    "          normalized_train_records['exercise_id'].values,\n",
    "          svm_est_test_labels,\n",
    "          normalized_test_records['exercise_id'].values,\n",
    "          main_title=\"SVM merged\")\n",
    "\n",
    "### Your code ends here ###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='task4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 4.</b>\n",
    "\n",
    "Use features calculated for each modality in task 3. Choose base classifier for each modality from scikit-learn. Train classifiers for each modality feature presentations separately and combine the outputs in decision level. Evaluate and compare the result on testing dataset. Do the subtasks given as\n",
    "<br>\n",
    "<br> \n",
    "<p> <b>4.1</b> Use base classifiers of support vector machine (SVM) with RBF-kernel and AdaBoost classifier (with random_state=0). \n",
    "Normalize data between 0-1 using the training dataset. Train the base classifiers by tuning the model parameters (<i>C</i> parameter and RBF-kernel <i>gamma</i> in SVM as well as <i>n_estimators</i> and <i>learning_rate</i> in Adaboost) using 10-fold cross-validation on training dataset to find optimal set of parameters (hint: use GridSearchCV from scikit-learn). For grid search use the following values $C = [0.1, 1.0, 10.0, 100.0]$, $gamma=[0.1, 0.25, 0.5, 0.75, 1.0, 2.0]$, $n\\_estimators = [50, 100, 500, 1000]$, and $learning\\_rate = [0.1, 0.25, 0.5, 0.75,1.0]$. Choose the best parameters and train the classifiers for each modality on whole training dataset. Is there a possibility that classifiers will overfit to training data using this parameter selection strategy? If so, why? </p>\n",
    "<br>\n",
    "<p> <b>4.2</b> Predict probabilistic outputs of each trained classifier for both modalities using the test set. </p>\n",
    "<br>\n",
    "<p> <b>4.3</b> Combine the probabilistic outputs of different modalities by fixed classification rules: max, min, prod, and sum. Evaluate, compare, and analyse the final combined results using confusion matrices and F1 scores. Show results for each base classifier combinations (i.e., $SVM_{acc}+SVM_{depth}$, $AdaBoost_{acc}+AdaBoost_{depth}$, $SVM_{acc}+AdaBoost_{depth}$, $AdaBoost_{acc}+SVM_{depth}$)</p>\n",
    "<br>\n",
    "Document your work, evaluate the results, and analyse the outcomes in each subtasks 4.1-4.3.\n",
    "    \n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4.1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "class DataNormalizer:\n",
    "    #Normalize 2D np array data into 0-1 value space\n",
    "    def __init__(self) -> None:\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def fit(self,data) -> None:\n",
    "        self.min = np.min(data)\n",
    "        self.max = np.max(data)\n",
    "\n",
    "    def transform(self,data):\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "\n",
    "class GridClassifier:\n",
    "    def __init__(self, clf, params, kfold=10) -> None:\n",
    "         self.grid_search = GridSearchCV(clf, params,cv=kfold)\n",
    "\n",
    "    def fit(self, data, labels) -> None:\n",
    "        self.grid_search.fit(data, labels)\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.grid_search.predict(data)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "act_train = np.concatenate(train_records_merged.df_y.values)\n",
    "act_test = np.concatenate(test_records_merged.df_y.values)\n",
    "act_train_labels = np.array(train_records_merged.exercise_id.apply(lambda x: int(x)))\n",
    "act_test_labels = np.array(test_records_merged.exercise_id.apply(lambda x: int(x)))\n",
    "\n",
    "\n",
    "dc_train = np.concatenate(train_records_merged.df_x.values)\n",
    "dc_test = np.concatenate(test_records_merged.df_x.values)\n",
    "dc_train_labels = np.array(train_records_merged.exercise_id.apply(lambda x: int(x)))\n",
    "dc_test_labels = np.array(test_records_merged.exercise_id.apply(lambda x: int(x)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#act_train = np.concatenate(pca_act_training_records_reshaped[\"df\"].values)\n",
    "#act_test = np.concatenate(pca_act_testing_records_reshaped[\"df\"].values)\n",
    "#act_train_labels = np.array(training_records[training_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x)))\n",
    "#act_test_labels = np.array(testing_records[testing_records.sensor_code=='act'].exercise_id.apply(lambda x: int(x)))\n",
    "#\n",
    "#\n",
    "#dc_train = np.concatenate(pca_dc_train_records_reshaped[\"df\"].values)\n",
    "#dc_test = np.concatenate(pca_dc_test_records_reshaped[\"df\"].values)\n",
    "#dc_train_labels = np.array(training_records[training_records.sensor_code=='dc'].exercise_id.apply(lambda x: int(x)))\n",
    "#dc_test_labels = np.array(testing_records[testing_records.sensor_code=='dc'].exercise_id.apply(lambda x: int(x)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Normalize the data:\n",
    "act_normalizer = DataNormalizer()\n",
    "act_normalizer.fit(act_train)\n",
    "act_train = act_normalizer.transform(act_train)\n",
    "act_test = act_normalizer.transform(act_test)\n",
    "\n",
    "dc_normalizer = DataNormalizer()\n",
    "dc_normalizer.fit(dc_train)\n",
    "dc_train = dc_normalizer.transform(dc_train)\n",
    "dc_test = dc_normalizer.transform(dc_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SVM:\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def svm_classify(train_data, train_labels, test_data=None):\n",
    "    svm_cls = svm.SVC(kernel='rbf',random_state=0)\n",
    "    svm_params = {'C' : [0.1, 1.0, 10.0, 100.0], 'gamma' : [0.1, 0.25, 0.5, 0.75, 1.0, 2.0]}\n",
    "    svm_search = GridSearchCV(svm_cls, svm_params,scoring='f1_macro',cv=10)\n",
    "    svm_search.fit(train_data,train_labels)\n",
    "    svm_best = svm.SVC(C=svm_search.best_params_['C'], kernel='rbf',gamma=svm_search.best_params_['gamma'], random_state=0,probability=True)\n",
    "    svm_best.fit(train_data,train_labels)\n",
    "    #svm_pred_train = svm_best.predict(train_data)\n",
    "    #svm_pred_test = svm_best.predict(test_data)\n",
    "    return svm_best\n",
    "# ADA\n",
    "def ada_classify(train_data, train_labels, test_data=None):\n",
    "    ada_cls = AdaBoostClassifier(random_state=0)\n",
    "    ada_params = {'n_estimators' : [50, 100, 500, 1000], 'learning_rate' : [0.1, 0.25, 0.5, 0.75,1.0]}\n",
    "    ada_search = GridSearchCV(ada_cls, ada_params,scoring='f1_macro',cv=10)\n",
    "    ada_search.fit(train_data,train_labels)\n",
    "    ada_best = AdaBoostClassifier(n_estimators=ada_search.best_params_['n_estimators'], learning_rate=ada_search.best_params_['learning_rate'], random_state=0)\n",
    "    ada_best.fit(train_data,train_labels)\n",
    "    #ada_pred_train = ada_best.predict(train_data)\n",
    "    #ada_pred_test = ada_best.predict(test_data)\n",
    "    return ada_best\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Classification accelerometer data:\n",
    "act_svm_best = svm_classify(act_train,act_train_labels,act_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "act_ada_best = ada_classify(act_train,act_train_labels,act_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Classification depth camera data:\n",
    "dc_svm_best  = svm_classify(dc_train,dc_train_labels,dc_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dc_ada_best = ada_classify(dc_train,dc_train_labels,dc_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 4.2 Get the predictions:\n",
    "act_svm_pred_train = act_svm_best.predict(act_train)\n",
    "act_svm_pred_test = act_svm_best.predict(act_test)\n",
    "act_svm_proba_train = act_svm_best.predict_proba(act_train)\n",
    "act_svm_proba_test = act_svm_best.predict_proba(act_test)\n",
    "\n",
    "act_ada_pred_train = act_ada_best.predict(act_train)\n",
    "act_ada_pred_test = act_ada_best.predict(act_test)\n",
    "act_ada_proba_train = act_ada_best.predict_proba(act_train)\n",
    "act_ada_proba_test = act_ada_best.predict_proba(act_test)\n",
    "\n",
    "dc_svm_pred_train =  dc_svm_best.predict(dc_train)\n",
    "dc_svm_pred_test =   dc_svm_best.predict(dc_test)\n",
    "dc_svm_proba_train = dc_svm_best.predict_proba(dc_train)\n",
    "dc_svm_proba_test =  dc_svm_best.predict_proba(dc_test)\n",
    "\n",
    "dc_ada_pred_train =  dc_ada_best.predict(dc_train)\n",
    "dc_ada_pred_test =   dc_ada_best.predict(dc_test)\n",
    "dc_ada_proba_train = dc_ada_best.predict_proba(dc_train)\n",
    "dc_ada_proba_test =  dc_ada_best.predict_proba(dc_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Accelerometer with svm:\n",
    "visualize(act_svm_pred_train,act_train_labels,act_svm_pred_test,act_test_labels,main_title=\"Accelerometer sensor used with svm to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Accelerometer with ada boost:\n",
    "visualize(act_ada_pred_train,act_train_labels,act_ada_pred_test,act_test_labels,main_title=\"Accelerometer sensor used with ada boost to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Depth Camera with svm:\n",
    "visualize(dc_svm_pred_train,dc_train_labels,dc_svm_pred_test,dc_test_labels,main_title=\"Depth Camera sensor used with svm to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Depth Camera with ada boost:\n",
    "visualize(dc_ada_pred_train,dc_train_labels,dc_ada_pred_test,dc_test_labels,main_title=\"Depth Camera sensor used with ada boost to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#4.3\n",
    "def combine_probabilities(proba_a_train,proba_b_train, proba_a_test,proba_b_test):\n",
    "    train_combined = pd.DataFrame()\n",
    "    test_combined = pd.DataFrame()\n",
    "    #For Train set\n",
    "    train_combined['mean'] = np.argmax(np.mean(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['sum'] = np.argmax(np.sum(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['prod'] = np.argmax(np.prod(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['max'] = np.argmax(np.max(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['min'] = np.argmax(np.min(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    #For Test set\n",
    "    test_combined['mean'] = np.argmax(np.mean(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['sum'] = np.argmax(np.sum(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['prod'] = np.argmax(np.prod(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['max'] = np.argmax(np.max(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['min'] = np.argmax(np.min(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    return train_combined, test_combined\n",
    "\n",
    "def combine_visualize(combi_train,labels_train,combi_test,labels_test,combination_name):\n",
    "    print(\"Visualize \"+ combination_name + \" Results :\")\n",
    "    classify_rules = ['mean','sum','prod','max','min']\n",
    "    for rule in classify_rules:\n",
    "        sub_title = combination_name + \" with \"+ rule + \" rule:\"\n",
    "        visualize(combi_train[rule],labels_train,\n",
    "                  combi_test[rule],labels_test,sub_title,scale=0.5)\n",
    "\n",
    "ground_truth_train = act_train_labels # They are equal to the dc_train_labels\n",
    "ground_truth_test = act_test_labels\n",
    "\n",
    "# Combine predictions SVM-act and SVM-dc\n",
    "svm2_train, svm2_test = combine_probabilities(act_svm_proba_train,dc_svm_proba_train,\n",
    "                                              act_svm_proba_test,dc_svm_proba_test)\n",
    "# Combine predictions SVM-act and Ada-dc\n",
    "svmad_train, svmad_test = combine_probabilities(act_svm_proba_train,dc_ada_proba_train,\n",
    "                                              act_svm_proba_test,dc_ada_proba_test)\n",
    "# Combine predictions Ada-act and Ada-dc\n",
    "ada2_train, ada2_test = combine_probabilities(act_ada_proba_train,dc_ada_proba_train,\n",
    "                                              act_ada_proba_test,dc_ada_proba_test)\n",
    "# Combine predictions Ada-act and SVM-dc\n",
    "adasv_train, adasv_test = combine_probabilities(act_ada_proba_train,dc_ada_proba_train,\n",
    "                                              act_ada_proba_test,dc_ada_proba_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM:\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def svm_classify(train_data, train_labels, test_data=None, svm_params = None):\n",
    "    svm_cls = svm.SVC(kernel='rbf',random_state=0)\n",
    "    svm_params = {'C' : [0.1, 1.0, 10.0, 100.0], 'gamma' : [0.1, 0.25, 0.5, 0.75, 1.0, 2.0]} if svm_params is None else svm_params\n",
    "    svm_search = GridSearchCV(svm_cls, svm_params,scoring='f1_macro',cv=10)\n",
    "    svm_search.fit(train_data,train_labels)\n",
    "    svm_best = svm.SVC(C=svm_search.best_params_['C'], kernel='rbf',gamma=svm_search.best_params_['gamma'], random_state=0,probability=True)\n",
    "    svm_best.fit(train_data,train_labels)\n",
    "    #svm_pred_train = svm_best.predict(train_data)\n",
    "    #svm_pred_test = svm_best.predict(test_data)\n",
    "    return svm_best\n",
    "# ADA\n",
    "def ada_classify(train_data, train_labels, test_data=None):\n",
    "    ada_cls = AdaBoostClassifier(random_state=0)\n",
    "    ada_params = {'n_estimators' : [50, 100, 500, 1000], 'learning_rate' : [0.1, 0.25, 0.5, 0.75,1.0]}\n",
    "    ada_search = GridSearchCV(ada_cls, ada_params,scoring='f1_macro',cv=10)\n",
    "    ada_search.fit(train_data,train_labels)\n",
    "    ada_best = AdaBoostClassifier(n_estimators=ada_search.best_params_['n_estimators'], learning_rate=ada_search.best_params_['learning_rate'], random_state=0)\n",
    "    ada_best.fit(train_data,train_labels)\n",
    "    #ada_pred_train = ada_best.predict(train_data)\n",
    "    #ada_pred_test = ada_best.predict(test_data)\n",
    "    return ada_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Classification accelerometer data:\n",
    "act_svm_best = svm_classify(act_train,act_train_labels,act_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "act_ada_best = ada_classify(act_train,act_train_labels,act_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Classification depth camera data:\n",
    "dc_svm_best  = svm_classify(dc_train,dc_train_labels,dc_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dc_ada_best = ada_classify(dc_train,dc_train_labels,dc_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 4.2 Get the predictions:\n",
    "act_svm_pred_train = act_svm_best.predict(act_train)\n",
    "act_svm_pred_test = act_svm_best.predict(act_test)\n",
    "act_svm_proba_train = act_svm_best.predict_proba(act_train)\n",
    "act_svm_proba_test = act_svm_best.predict_proba(act_test)\n",
    "\n",
    "act_ada_pred_train = act_ada_best.predict(act_train)\n",
    "act_ada_pred_test = act_ada_best.predict(act_test)\n",
    "act_ada_proba_train = act_ada_best.predict_proba(act_train)\n",
    "act_ada_proba_test = act_ada_best.predict_proba(act_test)\n",
    "\n",
    "dc_svm_pred_train =  dc_svm_best.predict(dc_train)\n",
    "dc_svm_pred_test =   dc_svm_best.predict(dc_test)\n",
    "dc_svm_proba_train = dc_svm_best.predict_proba(dc_train)\n",
    "dc_svm_proba_test =  dc_svm_best.predict_proba(dc_test)\n",
    "\n",
    "dc_ada_pred_train =  dc_ada_best.predict(dc_train)\n",
    "dc_ada_pred_test =   dc_ada_best.predict(dc_test)\n",
    "dc_ada_proba_train = dc_ada_best.predict_proba(dc_train)\n",
    "dc_ada_proba_test =  dc_ada_best.predict_proba(dc_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Accelerometer with svm:\n",
    "visualize(act_svm_pred_train,act_train_labels,act_svm_pred_test,act_test_labels,main_title=\"Accelerometer sensor used with svm to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Accelerometer with ada boost:\n",
    "visualize(act_ada_pred_train,act_train_labels,act_ada_pred_test,act_test_labels,main_title=\"Accelerometer sensor used with ada boost to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Depth Camera with svm:\n",
    "visualize(dc_svm_pred_train,dc_train_labels,dc_svm_pred_test,dc_test_labels,main_title=\"Depth Camera sensor used with svm to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Depth Camera with ada boost:\n",
    "visualize(dc_ada_pred_train,dc_train_labels,dc_ada_pred_test,dc_test_labels,main_title=\"Depth Camera sensor used with ada boost to classify the exercises\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#4.3\n",
    "def combine_probabilities(proba_a_train,proba_b_train, proba_a_test,proba_b_test):\n",
    "    train_combined = pd.DataFrame()\n",
    "    test_combined = pd.DataFrame()\n",
    "    #For Train set\n",
    "    train_combined['mean'] = np.argmax(np.mean(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['sum'] = np.argmax(np.sum(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['prod'] = np.argmax(np.prod(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['max'] = np.argmax(np.max(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    train_combined['min'] = np.argmax(np.min(np.stack((proba_a_train,proba_b_train)),axis=0),axis=1) + 1\n",
    "    #For Test set\n",
    "    test_combined['mean'] = np.argmax(np.mean(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['sum'] = np.argmax(np.sum(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['prod'] = np.argmax(np.prod(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['max'] = np.argmax(np.max(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    test_combined['min'] = np.argmax(np.min(np.stack((proba_a_test,proba_b_test)),axis=0),axis=1) + 1\n",
    "    return train_combined, test_combined\n",
    "\n",
    "def combine_visualize(combi_train,labels_train,combi_test,labels_test,combination_name):\n",
    "    print(\"Visualize \"+ combination_name + \" Results :\")\n",
    "    classify_rules = ['mean','sum','prod','max','min']\n",
    "    for rule in classify_rules:\n",
    "        sub_title = combination_name + \" with \"+ rule + \" rule:\"\n",
    "        visualize(combi_train[rule],labels_train,\n",
    "                  combi_test[rule],labels_test,sub_title,scale=0.5)\n",
    "\n",
    "ground_truth_train = act_train_labels # They are equal to the dc_train_labels\n",
    "ground_truth_test = act_test_labels\n",
    "\n",
    "# Combine predictions SVM-act and SVM-dc\n",
    "svm2_train, svm2_test = combine_probabilities(act_svm_proba_train,dc_svm_proba_train,\n",
    "                                              act_svm_proba_test,dc_svm_proba_test)\n",
    "# Combine predictions SVM-act and Ada-dc\n",
    "svmad_train, svmad_test = combine_probabilities(act_svm_proba_train,dc_ada_proba_train,\n",
    "                                              act_svm_proba_test,dc_ada_proba_test)\n",
    "# Combine predictions Ada-act and Ada-dc\n",
    "ada2_train, ada2_test = combine_probabilities(act_ada_proba_train,dc_ada_proba_train,\n",
    "                                              act_ada_proba_test,dc_ada_proba_test)\n",
    "# Combine predictions Ada-act and SVM-dc\n",
    "adasv_train, adasv_test = combine_probabilities(act_ada_proba_train,dc_ada_proba_train,\n",
    "                                              act_ada_proba_test,dc_ada_proba_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time: \", end_time - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualize\n",
    "# Combine predictions SVM-act and SVM-dc\n",
    "combine_visualize(svm2_train,ground_truth_train,svm2_test,ground_truth_test,\"SVM-act and SVM-dc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine predictions SVM-act and Ada-dc\n",
    "combine_visualize(svmad_train,ground_truth_train,svmad_test,ground_truth_test,\"SVM-act and Ada-dc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine predictions Ada-act and Ada-dc\n",
    "combine_visualize(ada2_train,ground_truth_train,ada2_test,ground_truth_test,\"Ada-act and Ada-dc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine predictions Ada-act and SVM-dc\n",
    "combine_visualize(adasv_train,ground_truth_train,adasv_test,ground_truth_test,\"Ada-act and SVM-dc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4f13fd6c3cbb70d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 5. Bonus task: Multimodal biometric identification of persons (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e082ae9d74f6755a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='task5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <b>Assigment.</b> <b>Task 5.</b>\n",
    "\n",
    "Can you build a classifier that recognizes the person who is performing the exercise? Use same 10 person dataset and split it so that first 25% of each long exercise sequence is used for training and rest 75% of each sequence is used for testing the classifier. Use same 5 second windowing with 3 seconds overlap to prepare the examples. Note that, now the person identity is the class label instead of exercise type. Max. 10 points are given but you can earn points from partial solution, as well.\n",
    "<br> \n",
    "<br> \n",
    "<p> <b>5.1</b> Build a classifier to identify persons based on the features and one of the models given in task 4 (max. 5 points).</p>\n",
    "<br> \n",
    "<p> <b>5.2</b> Can you build your own solution (using new features, new classification model or different fusion approaches) to beat the approach in Task 5.1 ? (max. 5 points) </p>\n",
    "<br>  \n",
    "Document your work. Evaluate and compare the results using confusion matrix and F1 score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#splt the data into training and testing sets:\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "def data_resample(data,n_samples=125):\n",
    "    return data.df_y.apply(\n",
    "            lambda x: pd.DataFrame().assign(\n",
    "                act_0=signal.resample(x.acc_0,n_samples),\n",
    "                act_1=signal.resample(x.acc_1,n_samples),\n",
    "                act_2=signal.resample(x.acc_2,n_samples)))\n",
    "\n",
    "def filter_dataframe(data,ratio=0.25):\n",
    "    data_train = pd.DataFrame()\n",
    "    data_test = pd.DataFrame()\n",
    "    for subject in range(0,10):\n",
    "        for exercise in range(0,7):\n",
    "            reading = data[(data.subject_id == stringify_id(subject+1)) &\n",
    "                           (data.exercise_id == stringify_id(exercise+1))]\n",
    "            samples_number = reading.shape[0]\n",
    "            train_number = int(samples_number * ratio)\n",
    "            #test_number = samples_number - train_number\n",
    "            tr = data[(data.subject_id == stringify_id(subject+1)) & (data.exercise_id == stringify_id(exercise+1))].iloc[:train_number]\n",
    "            te = data[(data.subject_id == stringify_id(subject+1)) & (data.exercise_id == stringify_id(exercise+1))].iloc[train_number:]\n",
    "            data_train = pd.concat([data_train,tr])\n",
    "            data_test = pd.concat([data_test,te])\n",
    "    data_train.df_y = data_resample(data_train)\n",
    "    data_test.df_y = data_resample(data_test)\n",
    "    return data_train, data_test\n",
    "\n",
    "#Merge dc and act to one row (one act without dc is omitted)\n",
    "records_merged = df_records_windowed[df_records_windowed.sensor_code=='dc'].merge(df_records_windowed[df_records_windowed.sensor_code=='act'], on=['subject_id', 'exercise_id', 'trial', 'window_idx'])\n",
    "\n",
    "\n",
    "train_records, test_records = filter_dataframe(records_merged,ratio=0.25)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "records_merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1\n",
    "\n",
    "### Your code begins here ###\n",
    "\n",
    "# Standardize the Data:\n",
    "act_s = Standardizer()\n",
    "act_s.fit(train_records,df_name='df_y')\n",
    "train_records = act_s.transform(train_records,df_name='df_y')\n",
    "test_records = act_s.transform(test_records,df_name='df_y')\n",
    "\n",
    "dc_s = Standardizer()\n",
    "dc_s.fit(train_records,df_name='df_x')\n",
    "train_records = dc_s.transform(train_records,df_name='df_x')\n",
    "test_records = dc_s.transform(test_records,df_name='df_x')\n",
    "\n",
    "labels_train = np.array(train_records.exercise_id.apply(lambda x: int(x)))\n",
    "labels_test = np.array(test_records.exercise_id.apply(lambda x: int(x)))\n",
    "\n",
    "\n",
    "#ACT : -----------------\n",
    "# Apply PCA:\n",
    "n_components = 10\n",
    "pca = PcaActApplier(n_components)\n",
    "pca.fit(train_records['df_y'])\n",
    "act_pca_train = pca.transform(train_records['df_y'])\n",
    "act_pca_test = pca.transform(test_records['df_y'])\n",
    "\n",
    "#LDA:\n",
    "#n_components = 1\n",
    "#lda = LdaActApplier(n_components)\n",
    "#lda.fit(train_records['df_y'],labels_train)\n",
    "#act_lda_train = lda.transform(train_records['df_y'])\n",
    "#act_lda_test = lda.transform(test_records['df_y'])\n",
    "\n",
    "\n",
    "act_features = np.concatenate((act_pca_train[0],act_pca_train[1],act_pca_train[2],\n",
    "                               #act_lda_train[0],act_lda_train[1],act_lda_train[2],\n",
    "                               np.mean(act_pca_train[0],axis=1).reshape(-1,1), np.std(act_pca_train[0],axis=1).reshape(-1,1),\n",
    "                               np.mean(act_pca_train[1],axis=1).reshape(-1,1), np.std(act_pca_train[1],axis=1).reshape(-1,1),\n",
    "                               np.mean(act_pca_train[2],axis=1).reshape(-1,1), np.std(act_pca_train[2],axis=1).reshape(-1,1)\n",
    "                              ),axis=1)\n",
    "\n",
    "act_test_features = np.concatenate((act_pca_test[0],act_pca_test[1],act_pca_test[2],\n",
    "                                    #act_lda_test[0],act_lda_test[1],act_lda_test[2],\n",
    "                                    np.mean(act_pca_test[0],axis=1).reshape(-1,1), np.std(act_pca_test[0],axis=1).reshape(-1,1),\n",
    "                                    np.mean(act_pca_test[1],axis=1).reshape(-1,1), np.std(act_pca_test[1],axis=1).reshape(-1,1),\n",
    "                                    np.mean(act_pca_test[2],axis=1).reshape(-1,1), np.std(act_pca_test[2],axis=1).reshape(-1,1)\n",
    "                                   ),axis=1)\n",
    "# DC -------------------\n",
    "#PCA\n",
    "def concatenate_images(df):\n",
    "    return df.reshape(1, -1)\n",
    "\n",
    "pca_applier = PcaDcApplier(n_components)\n",
    "pca_applier.fit(train_records,df_name='df_x')\n",
    "pca_dc_training_records = pca_applier.transform(train_records,df_name='df_x')\n",
    "pca_dc_testing_records = pca_applier.transform(test_records,df_name='df_x')\n",
    "\n",
    "pca_dc_training_records_reshaped = deepcopy(pca_dc_training_records)\n",
    "pca_dc_training_records_reshaped[\"df_x\"] = pca_dc_training_records_reshaped[\"df_x\"].apply(concatenate_images)\n",
    "pca_dc_testing_records_reshaped = deepcopy(pca_dc_testing_records)\n",
    "pca_dc_testing_records_reshaped[\"df_x\"] = pca_dc_testing_records_reshaped[\"df_x\"].apply(concatenate_images)\n",
    "\n",
    "#LDA\n",
    "#lda_applier = LdaDcApplier(n_components)\n",
    "#lda_applier.fit(train_records,df_name='df_x')\n",
    "#\n",
    "#lda_dc_train_records = lda_applier.transform(train_records,df_name='df_x')\n",
    "#lda_dc_test_records = lda_applier.transform(test_records,df_name='df_x')\n",
    "#\n",
    "#lda_dc_train_records = deepcopy(lda_dc_train_records)\n",
    "#lda_dc_train_records[\"df_x\"] = lda_dc_train_records[\"df_x\"].apply(concatenate_images)\n",
    "#lda_dc_test_records = deepcopy(lda_dc_test_records)\n",
    "#lda_dc_test_records[\"df_x\"] = lda_dc_test_records[\"df_x\"].apply(concatenate_images)\n",
    "\n",
    "\n",
    "#concat_dc_train_records = concat_pca_lda(pca_dc_training_records_reshaped, lda_dc_train_records,df_name='df_x')\n",
    "#concat_dc_test_records = concat_pca_lda(pca_dc_testing_records_reshaped, lda_dc_test_records,df_name='df_x')\n",
    "\n",
    "\n",
    "\n",
    "dc_features = np.concatenate(pca_dc_training_records_reshaped['df_x'].values,axis=0)\n",
    "dc_test_features = np.concatenate(pca_dc_testing_records_reshaped['df_x'].values,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the data:\n",
    "act_normalizer = DataNormalizer()\n",
    "act_normalizer.fit(act_features)\n",
    "act_features = act_normalizer.transform(act_features)\n",
    "act_test_features = act_normalizer.transform(act_test_features)\n",
    "\n",
    "dc_normalizer = DataNormalizer()\n",
    "dc_normalizer.fit(dc_features)\n",
    "dc_features = dc_normalizer.transform(dc_features)\n",
    "dc_test_features = dc_normalizer.transform(dc_test_features)\n",
    "\n",
    "# Classification\n",
    "act_svm_best = svm_classify(act_features,labels_train,act_test_features)\n",
    "dc_svm_best  = svm_classify(dc_features,labels_train,dc_test_features)\n",
    "\n",
    "# 4.2 Get the predictions:\n",
    "act_svm_pred_train = act_svm_best.predict(act_features)\n",
    "act_svm_pred_test = act_svm_best.predict(act_test_features)\n",
    "act_svm_proba_train = act_svm_best.predict_proba(act_features)\n",
    "act_svm_proba_test = act_svm_best.predict_proba(act_test_features)\n",
    "\n",
    "dc_svm_pred_train =  dc_svm_best.predict(dc_features)\n",
    "dc_svm_pred_test =   dc_svm_best.predict(dc_test_features)\n",
    "dc_svm_proba_train = dc_svm_best.predict_proba(dc_features)\n",
    "dc_svm_proba_test =  dc_svm_best.predict_proba(dc_test_features)\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Accelerometer with svm:\n",
    "visualize(act_svm_pred_train, labels_train, act_svm_pred_test, labels_test,\n",
    "          main_title=\"Accelerometer sensor used with svm to classify the subjects\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation Depth Camera with svm:\n",
    "visualize(dc_svm_pred_train, labels_train, dc_svm_pred_test, labels_test,\n",
    "          main_title=\"Depth Camera sensor used with svm to classify the subjects\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Combine the sensours\n",
    "svm2_train, svm2_test = combine_probabilities(act_svm_proba_train,dc_svm_proba_train,\n",
    "                                              act_svm_proba_test,dc_svm_proba_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine predictions SVM-act and SVM-dc\n",
    "combine_visualize(svm2_train,labels_train,svm2_test,labels_test,\"SVM-act and SVM-dc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2\n",
    "\n",
    "### Your code begins here ###\n",
    "combined_features = np.concatenate((act_svm_proba_train,dc_svm_proba_train),axis=1)\n",
    "combined_features_test = np.concatenate((act_svm_proba_test,dc_svm_proba_test),axis=1)\n",
    "svm_params = {'C' : [0.1, 1.0, 10.0, 100.0], 'gamma' : [0.1, 0.25, 0.5, 0.75, 1.0, 2.0]}\n",
    "svm_meta = svm_classify(combined_features, labels_train, combined_features_test, svm_params = svm_params)\n",
    "\n",
    "print('C=',svm_meta.C)\n",
    "print('gamma=',svm_meta.gamma)\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "C = svm_meta.C\n",
    "gamma = svm_meta.gamma\n",
    "svm_meta = svm.SVC(kernel='rbf',random_state=0,gamma=gamma,C=C)\n",
    "svm_meta.fit(combined_features,labels_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_pred_train =  svm_meta.predict(combined_features)\n",
    "svm_pred_test =   svm_meta.predict(combined_features_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize(svm_pred_train,labels_train,svm_pred_test,labels_test,\"Meta SVM-act and SVM-dc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perc = Perceptron(tol=1e-3, random_state=0)\n",
    "perc.fit(combined_features, labels_train)\n",
    "perc_pred_train =  perc.predict(combined_features)\n",
    "perc_pred_test =   perc.predict(combined_features_test)\n",
    "visualize(perc_pred_train,labels_train,perc_pred_test,labels_test,\"Meta Perceptron\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=100)\n",
    "neigh.fit(combined_features, labels_train)\n",
    "neigh_pred_train =  neigh.predict(combined_features)\n",
    "neigh_pred_test =   neigh.predict(combined_features_test)\n",
    "visualize(neigh_pred_train,labels_train,neigh_pred_test,labels_test,\"Meta Perceptron\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
